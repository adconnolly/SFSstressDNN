Restoring modules from user's e2cnn
/burg/glab/users/ac5006/miniconda/envs/e2cnn/lib/python3.10/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:80: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1660087551192/work/aten/src/ATen/native/IndexingUtils.h:27.)
  full_mask[mask] = norms.to(torch.uint8)
/burg/glab/users/ac5006/miniconda/envs/e2cnn/lib/python3.10/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:80: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1660087551192/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1581.)
  full_mask[mask] = norms.to(torch.uint8)
/burg/glab/users/ac5006/DNStoLES/buoyancyAblation_CNextrap/C4-bExc-midGridReExtrap-local.py:147: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig1 = plt.figure(figsize = (20, 6))
cuda
C4_bOut2_midGridReExtrap_local_4x1026Re900_4x2052Re1800_
Train Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 44)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 590200 590400 590600 590800 ... 608000 609000 610000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 15)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 616000 617000 618000 619000 ... 628000 629000 630000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (109649, 6)
input shape should be (109649, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (109649, 12, 3, 3)
Test Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 3)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 1368000 1377000 1386000
Data variables:
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    tau12    (z, y, x, time) float64 ...
    tau13    (z, y, x, time) float64 ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (141312, 6)
input shape should be (141312, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (141312, 12, 3, 3)
Lossweights:
[  307747.15960827  1668695.72937077  7246700.59237586  1094518.42019907
 10163900.0130578   5063340.10468084]
0
[0.01]
LR:  None
train loss: 0.3037122347183454
validation loss: 0.8990244707674948
test loss: 0.9001435846202837
1
[0.001]
LR:  None
train loss: 0.2785135282955136
validation loss: 0.8455285977483936
test loss: 0.8440640223299254
2
[0.0001]
LR:  None
train loss: 0.27698336214083225
validation loss: 0.8411234523486538
test loss: 0.8370175731321217
3
[0.0001]
LR:  None
train loss: 0.2757743027215233
validation loss: 0.840595469484996
test loss: 0.8401271617482388
4
[0.0001]
LR:  None
train loss: 0.27522965547358164
validation loss: 0.838564162547117
test loss: 0.8372076348243652
5
[0.0001]
LR:  None
train loss: 0.2745453034053035
validation loss: 0.8373517342730992
test loss: 0.8360442660941151
6
[0.0001]
LR:  None
train loss: 0.2742508620927454
validation loss: 0.8345490051194253
test loss: 0.8337720434629776
7
[0.0001]
LR:  None
train loss: 0.2729701536889602
validation loss: 0.8333565775129737
test loss: 0.8311569301158152
8
[0.0001]
LR:  None
train loss: 0.2719030042569121
validation loss: 0.8324095657321177
test loss: 0.8323029504846694
9
[0.0001]
LR:  None
train loss: 0.27166612023563075
validation loss: 0.8294143193836446
test loss: 0.8290628080108032
10
[0.0001]
LR:  None
train loss: 0.2709317932238149
validation loss: 0.8282871442477637
test loss: 0.8265585346719646
11
[0.0001]
LR:  None
train loss: 0.2702747828926804
validation loss: 0.8262879440008287
test loss: 0.8246497744571446
12
[0.0001]
LR:  None
train loss: 0.26910823665907274
validation loss: 0.8245013099270144
test loss: 0.8233839287119337
13
[0.0001]
LR:  None
train loss: 0.26825776297805104
validation loss: 0.8236544356367815
test loss: 0.8219950205707358
14
[0.0001]
LR:  None
train loss: 0.26798821169343595
validation loss: 0.820600662964084
test loss: 0.816441215763542
15
[0.0001]
LR:  None
train loss: 0.2675527434842485
validation loss: 0.8189320445256761
test loss: 0.8178908472851263
16
[0.0001]
LR:  None
train loss: 0.26616740464576866
validation loss: 0.8171171190889954
test loss: 0.8142743543900152
17
[0.0001]
LR:  None
train loss: 0.2662811808129475
validation loss: 0.8147008249501762
test loss: 0.8123034080290097
18
[0.0001]
LR:  None
train loss: 0.2656398131791932
validation loss: 0.8121462307062906
test loss: 0.8132193555694173
19
[0.0001]
LR:  None
train loss: 0.2643038996020402
validation loss: 0.8118344281130012
test loss: 0.8083946619118798
20
[0.0001]
LR:  None
train loss: 0.26399853783878546
validation loss: 0.8096217608368825
test loss: 0.8087039550824593
21
[0.0001]
LR:  None
train loss: 0.26269372264243784
validation loss: 0.8078651548564999
test loss: 0.8066952680346966
22
[0.0001]
LR:  None
train loss: 0.26202055518494255
validation loss: 0.8054943224161034
test loss: 0.8060996980486025
23
[0.0001]
LR:  None
train loss: 0.2616790559725245
validation loss: 0.8040901476176885
test loss: 0.8028300956966042
24
[0.0001]
LR:  None
train loss: 0.26104340516160485
validation loss: 0.8020184053578319
test loss: 0.8023544669881838
25
[0.0001]
LR:  None
train loss: 0.2599336107383505
validation loss: 0.800755175801619
test loss: 0.8000220835479184
26
[0.0001]
LR:  None
train loss: 0.25973447498029684
validation loss: 0.8011558779009067
test loss: 0.8019983391340015
27
[0.0001]
LR:  None
train loss: 0.25890037217221806
validation loss: 0.7989703193430093
test loss: 0.7977422989243315
28
[0.0001]
LR:  None
train loss: 0.2579841609821761
validation loss: 0.797486850314903
test loss: 0.7969538536886237
29
[0.0001]
LR:  None
train loss: 0.2581725430097239
validation loss: 0.795823985350455
test loss: 0.7944206551464482
30
[0.0001]
LR:  None
train loss: 0.2577318081016054
validation loss: 0.7961580806851846
test loss: 0.7934784317643229
31
[0.0001]
LR:  None
train loss: 0.25739579959699727
validation loss: 0.7951891258312412
test loss: 0.7932958324511087
32
[0.0001]
LR:  None
train loss: 0.25708076061226215
validation loss: 0.7935078977337032
test loss: 0.7927576772908042
33
[0.0001]
LR:  None
train loss: 0.25597819680640665
validation loss: 0.7914435064955985
test loss: 0.7878940302527135
34
[0.0001]
LR:  None
train loss: 0.25615577437285925
validation loss: 0.7913130911139319
test loss: 0.7899511480674379
35
[0.0001]
LR:  None
train loss: 0.2557882783383297
validation loss: 0.7903679920665347
test loss: 0.7870591200878594
36
[0.0001]
LR:  None
train loss: 0.2544512033227956
validation loss: 0.7888666101151734
test loss: 0.7858990914481899
37
[0.0001]
LR:  None
train loss: 0.25461807410596565
validation loss: 0.7901442592811629
test loss: 0.7896935978112062
38
[0.0001]
LR:  None
train loss: 0.2541466134926531
validation loss: 0.7889051920212363
test loss: 0.7892952010666279
39
[0.0001]
LR:  None
train loss: 0.2535664920760125
validation loss: 0.7875613064990622
test loss: 0.7844076522384694
40
[0.0001]
LR:  None
train loss: 0.25336525253614
validation loss: 0.7857596963892783
test loss: 0.785438924307469
41
[0.0001]
LR:  None
train loss: 0.2533901374904352
validation loss: 0.7845070802183234
test loss: 0.7802580698056484
42
[0.0001]
LR:  None
train loss: 0.25273846723774074
validation loss: 0.7844482783791733
test loss: 0.7821031340514738
43
[0.0001]
LR:  None
train loss: 0.2524325927474217
validation loss: 0.7847080908386392
test loss: 0.7847083863547123
44
[0.0001]
LR:  None
train loss: 0.2524266904493458
validation loss: 0.78363117542817
test loss: 0.7796762016390474
45
[0.0001]
LR:  None
train loss: 0.2511236606339989
validation loss: 0.7829530616838418
test loss: 0.7835330014043898
46
[0.0001]
LR:  None
train loss: 0.25159561712320566
validation loss: 0.782316181887664
test loss: 0.7796745772582856
47
[0.0001]
LR:  None
train loss: 0.2508714796114044
validation loss: 0.7820924532651254
test loss: 0.7784249940687022
48
[0.0001]
LR:  None
train loss: 0.2504685494481355
validation loss: 0.7812659339765942
test loss: 0.7794400209875657
49
[0.0001]
LR:  None
train loss: 0.25031487745747566
validation loss: 0.7801622550949509
test loss: 0.7764068689625229
50
[0.0001]
LR:  None
train loss: 0.2493864552622298
validation loss: 0.7798822797045503
test loss: 0.779573002081268
51
[0.0001]
LR:  None
train loss: 0.25023407670390185
validation loss: 0.7796313751537236
test loss: 0.7775935535517351
52
[0.0001]
LR:  None
train loss: 0.24888558950172796
validation loss: 0.7789308966252981
test loss: 0.7765168899076108
53
[0.0001]
LR:  None
train loss: 0.24815300512608937
validation loss: 0.77689040279111
test loss: 0.7764429061640075
54
[0.0001]
LR:  None
train loss: 0.24866664456797463
validation loss: 0.7767538181341764
test loss: 0.7770931068834297
55
[0.0001]
LR:  None
train loss: 0.24765100132193332
validation loss: 0.7773619268447229
test loss: 0.7745256763489206
56
[0.0001]
LR:  None
train loss: 0.24752719520792113
validation loss: 0.7757598532511399
test loss: 0.7746339670861419
57
[0.0001]
LR:  None
train loss: 0.24698789324334586
validation loss: 0.7753713779675024
test loss: 0.7739693176836399
58
[0.0001]
LR:  None
train loss: 0.24701372902682184
validation loss: 0.7755532158305372
test loss: 0.7727652347727749
59
[0.0001]
LR:  None
train loss: 0.24630568720427956
validation loss: 0.7750540806648603
test loss: 0.7734754132599023
60
[0.0001]
LR:  None
train loss: 0.24652935774945597
validation loss: 0.7743825076759755
test loss: 0.7711497885804629
61
[0.0001]
LR:  None
train loss: 0.24574096512772
validation loss: 0.7722527229192504
test loss: 0.7686622945189323
62
[0.0001]
LR:  None
train loss: 0.24505984714227053
validation loss: 0.7714629887145237
test loss: 0.7694673092950151
63
[0.0001]
LR:  None
train loss: 0.24498247530177797
validation loss: 0.7721585148826026
test loss: 0.7715405766750086
64
[0.0001]
LR:  None
train loss: 0.24476863321118933
validation loss: 0.7703198547600436
test loss: 0.7685964858460087
65
[0.0001]
LR:  None
train loss: 0.2438926785594828
validation loss: 0.7706399679837436
test loss: 0.7683497116473691
66
[0.0001]
LR:  None
train loss: 0.2437126607581811
validation loss: 0.769262718964369
test loss: 0.7667359509327262
67
[0.0001]
LR:  None
train loss: 0.24360790283055447
validation loss: 0.7694974299800976
test loss: 0.7665416611567135
68
[0.0001]
LR:  None
train loss: 0.24261568666176828
validation loss: 0.7684520033096309
test loss: 0.7668121402290299
69
[0.0001]
LR:  None
train loss: 0.24243066973504512
validation loss: 0.7690271201090502
test loss: 0.7665429113173701
70
[0.0001]
LR:  None
train loss: 0.242523450413433
validation loss: 0.7664147407392768
test loss: 0.7644028097273281
71
[0.0001]
LR:  None
train loss: 0.2416330221925133
validation loss: 0.766418788944224
test loss: 0.7649579045129463
72
[0.0001]
LR:  None
train loss: 0.24120939386528892
validation loss: 0.7642995089501583
test loss: 0.7622683842294129
73
[0.0001]
LR:  None
train loss: 0.24090592219772108
validation loss: 0.7642636300213477
test loss: 0.7629332993499257
74
[0.0001]
LR:  None
train loss: 0.24083464664623289
validation loss: 0.7641515708501567
test loss: 0.7634854561813027
75
[0.0001]
LR:  None
train loss: 0.24009270882013695
validation loss: 0.7634826865694392
test loss: 0.7600772860031719
76
[0.0001]
LR:  None
train loss: 0.23945636564930356
validation loss: 0.7621566052111814
test loss: 0.7602024561987518
77
[0.0001]
LR:  None
train loss: 0.2396050224579877
validation loss: 0.7613013624194634
test loss: 0.7605337715703372
78
[0.0001]
LR:  None
train loss: 0.23866359518511088
validation loss: 0.7606514255021803
test loss: 0.7570675558869165
79
[0.0001]
LR:  None
train loss: 0.2386148298325569
validation loss: 0.7593988116906336
test loss: 0.7595596655683848
80
[0.0001]
LR:  None
train loss: 0.23816642069828614
validation loss: 0.7599190485874201
test loss: 0.7572657298404692
81
[0.0001]
LR:  None
train loss: 0.2377571187288029
validation loss: 0.7569732938214121
test loss: 0.755684878131886
82
[0.0001]
LR:  None
train loss: 0.23739098039634113
validation loss: 0.7575006014498352
test loss: 0.7542780113930024
83
[0.0001]
LR:  None
train loss: 0.2374367589777224
validation loss: 0.7590068421676227
test loss: 0.759431703233168
84
[0.0001]
LR:  None
train loss: 0.23655371482124843
validation loss: 0.7562465003285037
test loss: 0.7542556249752723
85
[0.0001]
LR:  None
train loss: 0.23650400041991068
validation loss: 0.7564037169385351
test loss: 0.7532488532727081
86
[0.0001]
LR:  None
train loss: 0.2364031666949001
validation loss: 0.7555725691243389
test loss: 0.7507927628592292
87
[0.0001]
LR:  None
train loss: 0.2354473418599071
validation loss: 0.7537269650944001
test loss: 0.7539939767822883
88
[0.0001]
LR:  None
train loss: 0.2347471437816251
validation loss: 0.7537162765423798
test loss: 0.7516180226763778
89
[0.0001]
LR:  None
train loss: 0.23481328137131804
validation loss: 0.7529404468387365
test loss: 0.7494796165248083
90
[0.0001]
LR:  None
train loss: 0.23455663220915585
validation loss: 0.7520642941513778
test loss: 0.7503440464301617
91
[0.0001]
LR:  None
train loss: 0.2343860632913895
validation loss: 0.7504788510968242
test loss: 0.7511151100528174
92
[0.0001]
LR:  None
train loss: 0.23396134552333975
validation loss: 0.7508801217133598
test loss: 0.751377987770179
93
[0.0001]
LR:  None
train loss: 0.23362955721616302
validation loss: 0.7518044672053643
test loss: 0.7496042382029079
94
[0.0001]
LR:  None
train loss: 0.23362465392858192
validation loss: 0.7501562399125764
test loss: 0.748567402801583
95
[0.0001]
LR:  None
train loss: 0.23323871912373814
validation loss: 0.7506905413401643
test loss: 0.7492384225282943
96
[0.0001]
LR:  None
train loss: 0.23299524896698476
validation loss: 0.7502878874141217
test loss: 0.7478996073812356
97
[0.0001]
LR:  None
train loss: 0.2330731619825119
validation loss: 0.7493878567533985
test loss: 0.7476132429867566
98
[0.0001]
LR:  None
train loss: 0.23251127825178322
validation loss: 0.7502854282711143
test loss: 0.7498860443539588
99
[0.0001]
LR:  None
train loss: 0.23220275160402168
validation loss: 0.7479970220976337
test loss: 0.747295031715162
100
[0.0001]
LR:  None
train loss: 0.23141238388165308
validation loss: 0.747285928904822
test loss: 0.7462699606814613
101
[0.0001]
LR:  None
train loss: 0.23165449104743047
validation loss: 0.7459851095869212
test loss: 0.7454828670315108
102
[0.0001]
LR:  None
train loss: 0.23128538268443619
validation loss: 0.7479605834508521
test loss: 0.7436157497234047
103
[0.0001]
LR:  None
train loss: 0.23147691476806004
validation loss: 0.7470346328760115
test loss: 0.7454136163700078
104
[0.0001]
LR:  None
train loss: 0.2303032712566029
validation loss: 0.7467598521689152
test loss: 0.7432943753409568
105
[0.0001]
LR:  None
train loss: 0.22983460721354732
validation loss: 0.7448244027651847
test loss: 0.7457584610750114
106
[0.0001]
LR:  None
train loss: 0.2303335752092138
validation loss: 0.7426022634561746
test loss: 0.7405227751010205
107
[0.0001]
LR:  None
train loss: 0.23007996567511863
validation loss: 0.7444522919658222
test loss: 0.74274160978879
108
[0.0001]
LR:  None
train loss: 0.2291964379202157
validation loss: 0.7437808523314445
test loss: 0.7445206678607259
109
[0.0001]
LR:  None
train loss: 0.22912054051693254
validation loss: 0.7433275958474994
test loss: 0.7383488926344163
110
[0.0001]
LR:  None
train loss: 0.22886162567778814
validation loss: 0.7434664265005649
test loss: 0.7420867244953108
111
[0.0001]
LR:  None
train loss: 0.22851362666835165
validation loss: 0.7445439446519423
test loss: 0.7455938536711191
112
[0.0001]
LR:  None
train loss: 0.2284719573031721
validation loss: 0.7426870224382663
test loss: 0.7400149113996383
113
[0.0001]
LR:  None
train loss: 0.22815953260247157
validation loss: 0.7435045621553745
test loss: 0.7428533254619503
114
[0.0001]
LR:  None
train loss: 0.22812836542312973
validation loss: 0.743608521487568
test loss: 0.7410242154891247
115
[0.0001]
LR:  None
train loss: 0.22771191675416722
validation loss: 0.7432209761702437
test loss: 0.7404107632844568
116
[0.0001]
LR:  None
train loss: 0.2277323679597556
validation loss: 0.7425971044363261
test loss: 0.740271321010756
117
[0.0001]
LR:  None
train loss: 0.2274836898716302
validation loss: 0.743067016423017
test loss: 0.7406376269705248
118
[0.0001]
LR:  None
train loss: 0.22716010686443436
validation loss: 0.7440919368045689
test loss: 0.7413816913441049
119
[0.0001]
LR:  None
train loss: 0.22685972330442433
validation loss: 0.7425863051977286
test loss: 0.7434005978226363
120
[0.0001]
LR:  None
train loss: 0.22650471055855637
validation loss: 0.7418012927749473
test loss: 0.7395171433595618
121
[0.0001]
LR:  None
train loss: 0.22625846790817747
validation loss: 0.742477012127931
test loss: 0.7429080580423226
122
[0.0001]
LR:  None
train loss: 0.22631414877852596
validation loss: 0.7421013181046262
test loss: 0.7424250671455975
123
[0.0001]
LR:  None
train loss: 0.22594354562847807
validation loss: 0.7409435892461126
test loss: 0.7412525491610381
124
[0.0001]
LR:  None
train loss: 0.22596171050625335
validation loss: 0.7413273477173387
test loss: 0.7425935273692374
125
[0.0001]
LR:  None
train loss: 0.2258243921034593
validation loss: 0.7403860685176462
test loss: 0.7402590987309037
126
[0.0001]
LR:  None
train loss: 0.22563024810745808
validation loss: 0.7399353734462739
test loss: 0.7404481828305757
127
[0.0001]
LR:  None
train loss: 0.22515901086342416
validation loss: 0.7409100698258877
test loss: 0.7390247396005735
128
[0.0001]
LR:  None
train loss: 0.22468921417566343
validation loss: 0.7409160998405142
test loss: 0.7412849510081171
129
[0.0001]
LR:  None
train loss: 0.22417184221437592
validation loss: 0.7405974377878142
test loss: 0.7385659851001515
130
[0.0001]
LR:  None
train loss: 0.2243617496943091
validation loss: 0.7408607761489164
test loss: 0.7389400897453537
131
[0.0001]
LR:  None
train loss: 0.22419303456713066
validation loss: 0.7409898786128731
test loss: 0.7389716614841199
132
[0.0001]
LR:  None
train loss: 0.22425047480671947
validation loss: 0.7402118470267105
test loss: 0.7417854694433582
133
[0.0001]
LR:  None
train loss: 0.22369766527785698
validation loss: 0.7413055107841967
test loss: 0.7399956166383955
134
[0.0001]
LR:  None
train loss: 0.22415077368426573
validation loss: 0.740290597022013
test loss: 0.7377118509207705
135
[0.0001]
LR:  None
train loss: 0.22321963416448798
validation loss: 0.7402439759651896
test loss: 0.7399606968920691
136
[0.0001]
LR:  None
train loss: 0.22340039500952347
validation loss: 0.7403609628364386
test loss: 0.7376375589133571
137
[0.0001]
LR:  None
train loss: 0.2230923891920416
validation loss: 0.7418982524131676
test loss: 0.7398603973986863
138
[0.0001]
LR:  None
train loss: 0.22313836245113783
validation loss: 0.740290468525858
test loss: 0.7369349793691798
139
[0.0001]
LR:  None
train loss: 0.22237975605786525
validation loss: 0.7421480063706072
test loss: 0.7408443830735898
140
[0.0001]
LR:  None
train loss: 0.2224414822181861
validation loss: 0.7409853711969558
test loss: 0.7393001482992796
141
[0.0001]
LR:  None
train loss: 0.22265798521914204
validation loss: 0.7424517026946448
test loss: 0.741401239896082
142
[0.0001]
LR:  None
train loss: 0.2227417454852763
validation loss: 0.7410669342477957
test loss: 0.7424365343154111
143
[0.0001]
LR:  None
train loss: 0.22156064948021723
validation loss: 0.7419886181956008
test loss: 0.740306527945553
144
[0.0001]
LR:  None
train loss: 0.22209261172480582
validation loss: 0.7423927240336915
test loss: 0.742676811365949
145
[0.0001]
LR:  None
train loss: 0.22207039499174616
validation loss: 0.7416379150057636
test loss: 0.7452197848237577
146
[0.0001]
LR:  None
train loss: 0.22171817265523722
validation loss: 0.7412497726265994
test loss: 0.7412008721182988
ES epoch: 126
Test data
Skills for tau_11
R^2: 0.9334
Correlation: 0.9676

Skills for tau_12
R^2: 0.6927
Correlation: 0.8365

Skills for tau_13
R^2: 0.7414
Correlation: 0.8662

Skills for tau_22
R^2: 0.7768
Correlation: 0.8861

Skills for tau_23
R^2: 0.6876
Correlation: 0.8340

Skills for tau_33
R^2: 0.6596
Correlation: 0.8374

Validation data
Skills for tau_11
R^2: 0.9304
Correlation: 0.9662

Skills for tau_12
R^2: 0.6872
Correlation: 0.8330

Skills for tau_13
R^2: 0.7360
Correlation: 0.8633

Skills for tau_22
R^2: 0.7698
Correlation: 0.8827

Skills for tau_23
R^2: 0.6824
Correlation: 0.8312

Skills for tau_33
R^2: 0.6701
Correlation: 0.8418

Train data
Skills for tau_11
R^2: 0.9719
Correlation: 0.9866

Skills for tau_12
R^2: 0.8423
Correlation: 0.9185

Skills for tau_13
R^2: 0.6787
Correlation: 0.8269

Skills for tau_22
R^2: 0.8636
Correlation: 0.9322

Skills for tau_23
R^2: 0.6876
Correlation: 0.8303

Skills for tau_33
R^2: 0.3105
Correlation: 0.5835

Train Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 44)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 590200 590400 590600 590800 ... 608000 609000 610000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 15)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 616000 617000 618000 619000 ... 628000 629000 630000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (109286, 6)
input shape should be (109286, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (109286, 12, 3, 3)
Test Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 3)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 1368000 1377000 1386000
Data variables:
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    tau12    (z, y, x, time) float64 ...
    tau13    (z, y, x, time) float64 ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (141312, 6)
input shape should be (141312, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (141312, 12, 3, 3)
Lossweights:
[  310144.851   1672502.1973  7375109.6196  1106019.7496 10248187.2326  5329764.3475]
0
[0.01]
LR:  None
train loss: 0.29237381630801784
validation loss: 0.8873778285282766
test loss: 0.8803573199765176
1
[0.001]
LR:  None
train loss: 0.28072758426081806
validation loss: 0.863009261929196
test loss: 0.8571116194155475
2
[0.0001]
LR:  None
train loss: 0.2792859272159991
validation loss: 0.8599601139684794
test loss: 0.8541265237980921
3
[0.0001]
LR:  None
train loss: 0.27873137064876724
validation loss: 0.8579571891645308
test loss: 0.8518295734066458
4
[0.0001]
LR:  None
train loss: 0.2781175467929729
validation loss: 0.8568972761227582
test loss: 0.8504116181790974
5
[0.0001]
LR:  None
train loss: 0.2775078927289445
validation loss: 0.8565058075712084
test loss: 0.8501279842942461
6
[0.0001]
LR:  None
train loss: 0.27689849906592623
validation loss: 0.8535453403421661
test loss: 0.8475923776960546
7
[0.0001]
LR:  None
train loss: 0.2761774059526149
validation loss: 0.853583831827512
test loss: 0.8470860968404174
8
[0.0001]
LR:  None
train loss: 0.2755698554820747
validation loss: 0.8521078007650883
test loss: 0.8456343125642078
9
[0.0001]
LR:  None
train loss: 0.2749620142522222
validation loss: 0.8491563833711991
test loss: 0.8438683613618757
10
[0.0001]
LR:  None
train loss: 0.27418643860511416
validation loss: 0.8482800621588773
test loss: 0.8426924104566932
11
[0.0001]
LR:  None
train loss: 0.27363494616788087
validation loss: 0.8464422824760937
test loss: 0.8412731629016311
12
[0.0001]
LR:  None
train loss: 0.27289221133356834
validation loss: 0.8459558885806066
test loss: 0.8399496020483035
13
[0.0001]
LR:  None
train loss: 0.27224811046751335
validation loss: 0.8434340124279165
test loss: 0.8381107734405704
14
[0.0001]
LR:  None
train loss: 0.2715627611160106
validation loss: 0.8421193383021026
test loss: 0.8362138505744768
15
[0.0001]
LR:  None
train loss: 0.2709999182761701
validation loss: 0.8423614113820156
test loss: 0.8363073739848945
16
[0.0001]
LR:  None
train loss: 0.27036158433572954
validation loss: 0.8384499441874128
test loss: 0.8324320369355653
17
[0.0001]
LR:  None
train loss: 0.26967948978827144
validation loss: 0.8391362135881648
test loss: 0.8324254841924222
18
[0.0001]
LR:  None
train loss: 0.26905463772383453
validation loss: 0.8358818750247604
test loss: 0.8305925773183268
19
[0.0001]
LR:  None
train loss: 0.2685587436713988
validation loss: 0.8342537061856579
test loss: 0.8296040526317051
20
[0.0001]
LR:  None
train loss: 0.26782491691161187
validation loss: 0.833543103703624
test loss: 0.8284669151294397
21
[0.0001]
LR:  None
train loss: 0.26725315394107874
validation loss: 0.8326589885084517
test loss: 0.8269993487253268
22
[0.0001]
LR:  None
train loss: 0.26676195613214376
validation loss: 0.8318572860997387
test loss: 0.8262525195080909
23
[0.0001]
LR:  None
train loss: 0.2660509466401779
validation loss: 0.8302203971428119
test loss: 0.8243935465841715
24
[0.0001]
LR:  None
train loss: 0.2654739379430746
validation loss: 0.82848474795805
test loss: 0.8226935122171367
25
[0.0001]
LR:  None
train loss: 0.26482631689188635
validation loss: 0.8267055288384206
test loss: 0.8220495357341939
26
[0.0001]
LR:  None
train loss: 0.26420446698169564
validation loss: 0.825096358384538
test loss: 0.8196259946321176
27
[0.0001]
LR:  None
train loss: 0.2635557472168057
validation loss: 0.8235679266793031
test loss: 0.8182581898669141
28
[0.0001]
LR:  None
train loss: 0.26310351667654663
validation loss: 0.8226807923063738
test loss: 0.8174861756406727
29
[0.0001]
LR:  None
train loss: 0.26232837612824506
validation loss: 0.8212331927856769
test loss: 0.8156088383016322
30
[0.0001]
LR:  None
train loss: 0.2618482486745124
validation loss: 0.8196468454717784
test loss: 0.8137174885145919
31
[0.0001]
LR:  None
train loss: 0.2610611672432713
validation loss: 0.8162272521781639
test loss: 0.8114114341991446
32
[0.0001]
LR:  None
train loss: 0.2604534438624926
validation loss: 0.8161635562585423
test loss: 0.8102298652763107
33
[0.0001]
LR:  None
train loss: 0.2598113976301741
validation loss: 0.8143338496754986
test loss: 0.8087437547597562
34
[0.0001]
LR:  None
train loss: 0.25933258763696376
validation loss: 0.8129230484001184
test loss: 0.8075638007706377
35
[0.0001]
LR:  None
train loss: 0.25880005229160163
validation loss: 0.8098843342224472
test loss: 0.8046980491174803
36
[0.0001]
LR:  None
train loss: 0.2580497736830883
validation loss: 0.8087576635327548
test loss: 0.8041064731461546
37
[0.0001]
LR:  None
train loss: 0.25745640848270357
validation loss: 0.8068270058968864
test loss: 0.8020004313803374
38
[0.0001]
LR:  None
train loss: 0.2568386585777106
validation loss: 0.8058753435338775
test loss: 0.8009720382734634
39
[0.0001]
LR:  None
train loss: 0.25639353636850415
validation loss: 0.8057262095001414
test loss: 0.8006223486663583
40
[0.0001]
LR:  None
train loss: 0.25591595707770803
validation loss: 0.8044392590952149
test loss: 0.798445147669579
41
[0.0001]
LR:  None
train loss: 0.25549551877661747
validation loss: 0.8030847500027437
test loss: 0.7982552281704866
42
[0.0001]
LR:  None
train loss: 0.2548946705119405
validation loss: 0.8033728881022753
test loss: 0.7978106773684547
43
[0.0001]
LR:  None
train loss: 0.2544601889611647
validation loss: 0.8004154688505258
test loss: 0.7960446349643819
44
[0.0001]
LR:  None
train loss: 0.2541026001342388
validation loss: 0.800142488805774
test loss: 0.795239259416097
45
[0.0001]
LR:  None
train loss: 0.25372432974266285
validation loss: 0.7993434186388991
test loss: 0.795003736360457
46
[0.0001]
LR:  None
train loss: 0.25326602098946943
validation loss: 0.798266945873316
test loss: 0.792668201278476
47
[0.0001]
LR:  None
train loss: 0.2528526787829988
validation loss: 0.7977225069409062
test loss: 0.7923866635922971
48
[0.0001]
LR:  None
train loss: 0.252455494934093
validation loss: 0.7965508683867544
test loss: 0.7920730435360132
49
[0.0001]
LR:  None
train loss: 0.2520213517898612
validation loss: 0.7962225850346828
test loss: 0.791172274586016
50
[0.0001]
LR:  None
train loss: 0.25154671838456344
validation loss: 0.796508084350991
test loss: 0.7911454377803786
51
[0.0001]
LR:  None
train loss: 0.2512938607478035
validation loss: 0.7950463285972472
test loss: 0.7898123502709529
52
[0.0001]
LR:  None
train loss: 0.25102925946887655
validation loss: 0.7942261748825267
test loss: 0.7898654595891625
53
[0.0001]
LR:  None
train loss: 0.2505174174447826
validation loss: 0.7946258397595739
test loss: 0.7892899957622734
54
[0.0001]
LR:  None
train loss: 0.25009212291260907
validation loss: 0.7945519948799663
test loss: 0.7890882168037133
55
[0.0001]
LR:  None
train loss: 0.2497114231753806
validation loss: 0.7918006150343518
test loss: 0.7874634304388549
56
[0.0001]
LR:  None
train loss: 0.2493215104058098
validation loss: 0.792123866279415
test loss: 0.7871801999015344
57
[0.0001]
LR:  None
train loss: 0.24891963654701615
validation loss: 0.7907244880153175
test loss: 0.7862600314357197
58
[0.0001]
LR:  None
train loss: 0.24863166821927826
validation loss: 0.7900839149056675
test loss: 0.7854755280113965
59
[0.0001]
LR:  None
train loss: 0.24825002409331814
validation loss: 0.7894765167872189
test loss: 0.7846905688928562
60
[0.0001]
LR:  None
train loss: 0.24788437016944567
validation loss: 0.7898071652149766
test loss: 0.7843677991575636
61
[0.0001]
LR:  None
train loss: 0.24733752309143264
validation loss: 0.7880248773166619
test loss: 0.7834682647315616
62
[0.0001]
LR:  None
train loss: 0.24706787803869523
validation loss: 0.7881215629221767
test loss: 0.7835287175167867
63
[0.0001]
LR:  None
train loss: 0.24647920986044602
validation loss: 0.7867130029553692
test loss: 0.7823751068267438
64
[0.0001]
LR:  None
train loss: 0.24611341015587698
validation loss: 0.7863114306895274
test loss: 0.7822086808196134
65
[0.0001]
LR:  None
train loss: 0.2456067864721464
validation loss: 0.7836150149211042
test loss: 0.7793161629653136
66
[0.0001]
LR:  None
train loss: 0.2451221658026297
validation loss: 0.7836357325178407
test loss: 0.7797264313602713
67
[0.0001]
LR:  None
train loss: 0.24461624615259198
validation loss: 0.7813244876213725
test loss: 0.7777164678457928
68
[0.0001]
LR:  None
train loss: 0.24405513188426892
validation loss: 0.7810282355764773
test loss: 0.776503717425279
69
[0.0001]
LR:  None
train loss: 0.24341858847810136
validation loss: 0.7797545173951628
test loss: 0.7756155148618582
70
[0.0001]
LR:  None
train loss: 0.24309153561358002
validation loss: 0.7793603996127776
test loss: 0.7758913644171898
71
[0.0001]
LR:  None
train loss: 0.24248767917201752
validation loss: 0.7770095572399559
test loss: 0.7733490141501314
72
[0.0001]
LR:  None
train loss: 0.24211732251633783
validation loss: 0.7766244915738608
test loss: 0.7726966887152974
73
[0.0001]
LR:  None
train loss: 0.2413725322204492
validation loss: 0.7745760461302178
test loss: 0.7701158406226968
74
[0.0001]
LR:  None
train loss: 0.24095712968257843
validation loss: 0.7740056020619035
test loss: 0.7706346944181361
75
[0.0001]
LR:  None
train loss: 0.24037285015209842
validation loss: 0.7722609856044776
test loss: 0.7678309426580633
76
[0.0001]
LR:  None
train loss: 0.23991002823501495
validation loss: 0.7735081636341613
test loss: 0.7695784555495594
77
[0.0001]
LR:  None
train loss: 0.23949671954198698
validation loss: 0.7701868054164946
test loss: 0.7671329491355723
78
[0.0001]
LR:  None
train loss: 0.239025608362917
validation loss: 0.7703063784638801
test loss: 0.7674245003907302
79
[0.0001]
LR:  None
train loss: 0.2386938687093434
validation loss: 0.7694419381771495
test loss: 0.7653224818406553
80
[0.0001]
LR:  None
train loss: 0.23815266506428542
validation loss: 0.7697805656226849
test loss: 0.7655086193394034
81
[0.0001]
LR:  None
train loss: 0.23783182817670337
validation loss: 0.7673553552186202
test loss: 0.7633207827893581
82
[0.0001]
LR:  None
train loss: 0.23744278296613533
validation loss: 0.7658597025713423
test loss: 0.7631694941661804
83
[0.0001]
LR:  None
train loss: 0.23718678653248107
validation loss: 0.7684740266599086
test loss: 0.7640628946063757
84
[0.0001]
LR:  None
train loss: 0.23672045614718412
validation loss: 0.765998746359488
test loss: 0.7626695093307913
85
[0.0001]
LR:  None
train loss: 0.23623030146490917
validation loss: 0.7665532676878514
test loss: 0.7628243790210542
86
[0.0001]
LR:  None
train loss: 0.23589156336320333
validation loss: 0.7645475852020548
test loss: 0.7609918725159515
87
[0.0001]
LR:  None
train loss: 0.23548690551695173
validation loss: 0.7634033086496468
test loss: 0.7603726393856165
88
[0.0001]
LR:  None
train loss: 0.23522496540092144
validation loss: 0.7642782153556047
test loss: 0.7611249783787242
89
[0.0001]
LR:  None
train loss: 0.2348289436200832
validation loss: 0.7631891356784034
test loss: 0.7599014642381373
90
[0.0001]
LR:  None
train loss: 0.23448589291268238
validation loss: 0.7609665774670793
test loss: 0.7583017869831565
91
[0.0001]
LR:  None
train loss: 0.23446564427957792
validation loss: 0.7607135825645203
test loss: 0.7577043448882741
92
[0.0001]
LR:  None
train loss: 0.23414002983292717
validation loss: 0.7624408465566944
test loss: 0.7593357023905642
93
[0.0001]
LR:  None
train loss: 0.23369875349203095
validation loss: 0.7617392515736298
test loss: 0.7587486310188817
94
[0.0001]
LR:  None
train loss: 0.2334677464006026
validation loss: 0.7601558916381549
test loss: 0.7574915639403041
95
[0.0001]
LR:  None
train loss: 0.23302104156764933
validation loss: 0.7605135414042815
test loss: 0.7575264455730187
96
[0.0001]
LR:  None
train loss: 0.23276372829557096
validation loss: 0.7616595213018297
test loss: 0.7580189951770855
97
[0.0001]
LR:  None
train loss: 0.23269693110228196
validation loss: 0.7593556106130525
test loss: 0.756213099916007
98
[0.0001]
LR:  None
train loss: 0.23219510519079634
validation loss: 0.7602702329104086
test loss: 0.756419984553666
99
[0.0001]
LR:  None
train loss: 0.23212737098424815
validation loss: 0.7581680815347536
test loss: 0.7555087816138663
100
[0.0001]
LR:  None
train loss: 0.23178010821702566
validation loss: 0.7585038568699446
test loss: 0.7556455897394634
101
[0.0001]
LR:  None
train loss: 0.23149221681038637
validation loss: 0.7593034811696425
test loss: 0.7564592477077032
102
[0.0001]
LR:  None
train loss: 0.2312881273555642
validation loss: 0.7587995896647098
test loss: 0.7557692060374995
103
[0.0001]
LR:  None
train loss: 0.23099878107136254
validation loss: 0.7582663784862969
test loss: 0.7553971084410189
104
[0.0001]
LR:  None
train loss: 0.23055394327524545
validation loss: 0.7569720920776539
test loss: 0.7541563532112052
105
[0.0001]
LR:  None
train loss: 0.23032601171945943
validation loss: 0.7565023107688357
test loss: 0.7541088775570907
106
[0.0001]
LR:  None
train loss: 0.23020501834665294
validation loss: 0.7577893557705553
test loss: 0.7557772218675028
107
[0.0001]
LR:  None
train loss: 0.22990107336453114
validation loss: 0.7585309059821608
test loss: 0.7555978417816148
108
[0.0001]
LR:  None
train loss: 0.22973134206220924
validation loss: 0.7564393391139195
test loss: 0.7538600063907704
109
[0.0001]
LR:  None
train loss: 0.22945551940566572
validation loss: 0.7579898034989542
test loss: 0.7549296957604684
110
[0.0001]
LR:  None
train loss: 0.22908102112683354
validation loss: 0.7573334441086852
test loss: 0.7542822538747257
111
[0.0001]
LR:  None
train loss: 0.2287433868116822
validation loss: 0.7576740220405964
test loss: 0.7544568692191786
112
[0.0001]
LR:  None
train loss: 0.22874716686668722
validation loss: 0.7577265167084278
test loss: 0.7547424998386109
113
[0.0001]
LR:  None
train loss: 0.22850812939688767
validation loss: 0.7566950383697115
test loss: 0.7541630454449235
114
[0.0001]
LR:  None
train loss: 0.2282785474974975
validation loss: 0.7559803772035967
test loss: 0.7528514158641417
115
[0.0001]
LR:  None
train loss: 0.22832408411414912
validation loss: 0.7567236503085002
test loss: 0.7530548294141735
116
[0.0001]
LR:  None
train loss: 0.2276808216058034
validation loss: 0.7568948217644221
test loss: 0.7548050393571809
117
[0.0001]
LR:  None
train loss: 0.22756044087086427
validation loss: 0.7567970007177435
test loss: 0.7543999054746282
118
[0.0001]
LR:  None
train loss: 0.22713710435456327
validation loss: 0.7556433810859383
test loss: 0.7533176744669062
119
[0.0001]
LR:  None
train loss: 0.22702207802307026
validation loss: 0.7549906500193604
test loss: 0.7528223426224959
120
[0.0001]
LR:  None
train loss: 0.2267369552902039
validation loss: 0.7548977311279207
test loss: 0.7519485104820472
121
[0.0001]
LR:  None
train loss: 0.22649513651478229
validation loss: 0.7563264539528326
test loss: 0.7535793098884342
122
[0.0001]
LR:  None
train loss: 0.22636346655172093
validation loss: 0.7556879278708151
test loss: 0.7526839366826028
123
[0.0001]
LR:  None
train loss: 0.22616797948241915
validation loss: 0.7552699121111276
test loss: 0.7530188432136605
124
[0.0001]
LR:  None
train loss: 0.22604416241121872
validation loss: 0.754048897554149
test loss: 0.7519265844844447
125
[0.0001]
LR:  None
train loss: 0.22591410360367917
validation loss: 0.7545429134451701
test loss: 0.7520481826989777
126
[0.0001]
LR:  None
train loss: 0.22548582818946003
validation loss: 0.7543613424748815
test loss: 0.752158646847142
127
[0.0001]
LR:  None
train loss: 0.2253274485721915
validation loss: 0.7556878266662922
test loss: 0.7535034814130199
128
[0.0001]
LR:  None
train loss: 0.22491039246265906
validation loss: 0.7535893975552962
test loss: 0.7513500273495685
129
[0.0001]
LR:  None
train loss: 0.2246987988356467
validation loss: 0.7563646363266177
test loss: 0.75397245559103
130
[0.0001]
LR:  None
train loss: 0.22438966443040867
validation loss: 0.7543794813993577
test loss: 0.7519378866094218
131
[0.0001]
LR:  None
train loss: 0.22436349578321507
validation loss: 0.755660786981802
test loss: 0.7524560542423787
132
[0.0001]
LR:  None
train loss: 0.22418263854599443
validation loss: 0.7551972167263142
test loss: 0.7525318435558469
133
[0.0001]
LR:  None
train loss: 0.22419281538876265
validation loss: 0.7563108143475747
test loss: 0.7532218441594846
134
[0.0001]
LR:  None
train loss: 0.22380023399534785
validation loss: 0.7560942958267569
test loss: 0.7545194878564104
135
[0.0001]
LR:  None
train loss: 0.22362134809556994
validation loss: 0.7554431603352062
test loss: 0.751999317217874
136
[0.0001]
LR:  None
train loss: 0.22337086888349109
validation loss: 0.7559498788615304
test loss: 0.7531014252699273
137
[0.0001]
LR:  None
train loss: 0.22294817572171105
validation loss: 0.7554848955232599
test loss: 0.753287850914131
138
[0.0001]
LR:  None
train loss: 0.2227889982898042
validation loss: 0.7556610110220832
test loss: 0.7528090511930458
139
[0.0001]
LR:  None
train loss: 0.22268486825061995
validation loss: 0.7567605274719861
test loss: 0.7533844310055923
140
[0.0001]
LR:  None
train loss: 0.22250125878150082
validation loss: 0.7550154292094373
test loss: 0.7528872495857987
141
[0.0001]
LR:  None
train loss: 0.22235741635427966
validation loss: 0.7560416202952094
test loss: 0.7536926728953504
142
[0.0001]
LR:  None
train loss: 0.22215344406046716
validation loss: 0.7544060315807759
test loss: 0.7520893804837957
143
[0.0001]
LR:  None
train loss: 0.22183413433665236
validation loss: 0.7547345366456436
test loss: 0.7519658142331391
144
[0.0001]
LR:  None
train loss: 0.2216642572076422
validation loss: 0.7547910851685364
test loss: 0.7518825397363447
145
[0.0001]
LR:  None
train loss: 0.22147776685094997
validation loss: 0.7561484460482746
test loss: 0.7532431957993415
146
[0.0001]
LR:  None
train loss: 0.22123736666532487
validation loss: 0.7546114478381143
test loss: 0.7519050857213574
147
[0.0001]
LR:  None
train loss: 0.2210810839676227
validation loss: 0.7536574953073114
test loss: 0.7520514302019188
148
[0.0001]
LR:  None
train loss: 0.2208478677101963
validation loss: 0.7567546580477196
test loss: 0.7533677702101069
ES epoch: 128
Test data
Skills for tau_11
R^2: 0.9318
Correlation: 0.9672

Skills for tau_12
R^2: 0.6969
Correlation: 0.8381

Skills for tau_13
R^2: 0.7330
Correlation: 0.8608

Skills for tau_22
R^2: 0.7774
Correlation: 0.8860

Skills for tau_23
R^2: 0.6849
Correlation: 0.8324

Skills for tau_33
R^2: 0.6519
Correlation: 0.8348

Validation data
Skills for tau_11
R^2: 0.9304
Correlation: 0.9667

Skills for tau_12
R^2: 0.6832
Correlation: 0.8303

Skills for tau_13
R^2: 0.7446
Correlation: 0.8672

Skills for tau_22
R^2: 0.7814
Correlation: 0.8877

Skills for tau_23
R^2: 0.6746
Correlation: 0.8268

Skills for tau_33
R^2: 0.6556
Correlation: 0.8375

Train data
Skills for tau_11
R^2: 0.9732
Correlation: 0.9868

Skills for tau_12
R^2: 0.8541
Correlation: 0.9250

Skills for tau_13
R^2: 0.6836
Correlation: 0.8290

Skills for tau_22
R^2: 0.8659
Correlation: 0.9329

Skills for tau_23
R^2: 0.7093
Correlation: 0.8441

Skills for tau_33
R^2: 0.3517
Correlation: 0.6165

Train Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 44)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 590200 590400 590600 590800 ... 608000 609000 610000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 15)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 616000 617000 618000 619000 ... 628000 629000 630000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (108759, 6)
input shape should be (108759, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (108759, 12, 3, 3)
Test Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 3)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 1368000 1377000 1386000
Data variables:
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    tau12    (z, y, x, time) float64 ...
    tau13    (z, y, x, time) float64 ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (141312, 6)
input shape should be (141312, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (141312, 12, 3, 3)
Lossweights:
[  309534.4507  1666600.5211  7317522.8711  1093235.8658 10200469.4531  5281646.7089]
0
[0.01]
LR:  None
train loss: 0.30043955080540546
validation loss: 0.9082594693856777
test loss: 0.8963124795994779
1
[0.001]
LR:  None
train loss: 0.27669423939296856
validation loss: 0.8515560931776296
test loss: 0.8430401398064068
2
[0.0001]
LR:  None
train loss: 0.2754520532889267
validation loss: 0.8474341007484261
test loss: 0.8365320986022254
3
[0.0001]
LR:  None
train loss: 0.2749097664947304
validation loss: 0.8457688324250622
test loss: 0.8372270324455613
4
[0.0001]
LR:  None
train loss: 0.2739280831980556
validation loss: 0.8438022963536163
test loss: 0.8366460274949574
5
[0.0001]
LR:  None
train loss: 0.2731347282095286
validation loss: 0.8422929137084919
test loss: 0.8342586642557771
6
[0.0001]
LR:  None
train loss: 0.272509592069266
validation loss: 0.8408176095777722
test loss: 0.8293381510540168
7
[0.0001]
LR:  None
train loss: 0.27170332735815034
validation loss: 0.8409705690428065
test loss: 0.8319608247413609
8
[0.0001]
LR:  None
train loss: 0.2708223516876207
validation loss: 0.837184181483806
test loss: 0.8253866114134012
9
[0.0001]
LR:  None
train loss: 0.2703462827310744
validation loss: 0.8377803459220231
test loss: 0.8280153008501547
10
[0.0001]
LR:  None
train loss: 0.2695697087778424
validation loss: 0.8354421219770855
test loss: 0.8289456540476361
11
[0.0001]
LR:  None
train loss: 0.2691182148930163
validation loss: 0.8332916936162986
test loss: 0.8255560379345038
12
[0.0001]
LR:  None
train loss: 0.26827554648392593
validation loss: 0.8329827822577749
test loss: 0.8239402698320945
13
[0.0001]
LR:  None
train loss: 0.2673127084520533
validation loss: 0.830180683562596
test loss: 0.8192324338689799
14
[0.0001]
LR:  None
train loss: 0.2664973165442251
validation loss: 0.8272640327454069
test loss: 0.8164794571477954
15
[0.0001]
LR:  None
train loss: 0.2659965004226803
validation loss: 0.8262539659955263
test loss: 0.8177967387192857
16
[0.0001]
LR:  None
train loss: 0.2654032316805609
validation loss: 0.8252107704304782
test loss: 0.8169127355421001
17
[0.0001]
LR:  None
train loss: 0.26485460933876054
validation loss: 0.8236160996336727
test loss: 0.8159412500753633
18
[0.0001]
LR:  None
train loss: 0.26410027040469825
validation loss: 0.8210610887231061
test loss: 0.8117279494857563
19
[0.0001]
LR:  None
train loss: 0.263248572446582
validation loss: 0.8209103717107017
test loss: 0.8103423354274153
20
[0.0001]
LR:  None
train loss: 0.26246490310196974
validation loss: 0.8172638177055789
test loss: 0.807443968459439
21
[0.0001]
LR:  None
train loss: 0.2621452444913075
validation loss: 0.8160844777370716
test loss: 0.8060308268298286
22
[0.0001]
LR:  None
train loss: 0.2610586719574039
validation loss: 0.8151922475738933
test loss: 0.8057716356476678
23
[0.0001]
LR:  None
train loss: 0.260666465494615
validation loss: 0.8126447679093867
test loss: 0.8025680494964664
24
[0.0001]
LR:  None
train loss: 0.25979197593308107
validation loss: 0.8102560784815647
test loss: 0.8046285267375852
25
[0.0001]
LR:  None
train loss: 0.2589137485075626
validation loss: 0.8096695852701024
test loss: 0.797192508090939
26
[0.0001]
LR:  None
train loss: 0.25861295862193673
validation loss: 0.8072923150923333
test loss: 0.79801104117728
27
[0.0001]
LR:  None
train loss: 0.25785534784325603
validation loss: 0.804999343352158
test loss: 0.7927931084738985
28
[0.0001]
LR:  None
train loss: 0.25720132394210254
validation loss: 0.8063433073744383
test loss: 0.7949091519949426
29
[0.0001]
LR:  None
train loss: 0.2564733397200435
validation loss: 0.8019835676565644
test loss: 0.7923639021982466
30
[0.0001]
LR:  None
train loss: 0.2559784202640094
validation loss: 0.8000583073591169
test loss: 0.7883373313305548
31
[0.0001]
LR:  None
train loss: 0.2548840490689733
validation loss: 0.7992546933098412
test loss: 0.7917906856798325
32
[0.0001]
LR:  None
train loss: 0.25481139290392674
validation loss: 0.7992003916194141
test loss: 0.7894807599194468
33
[0.0001]
LR:  None
train loss: 0.2544271621297319
validation loss: 0.7965800766874328
test loss: 0.7876938875660527
34
[0.0001]
LR:  None
train loss: 0.25355386134219693
validation loss: 0.7961235823877351
test loss: 0.7866614115372818
35
[0.0001]
LR:  None
train loss: 0.2527907641703218
validation loss: 0.7925825309191553
test loss: 0.7806237932515797
36
[0.0001]
LR:  None
train loss: 0.2520591454043224
validation loss: 0.7917830493271888
test loss: 0.7829325792081272
37
[0.0001]
LR:  None
train loss: 0.2514182633235925
validation loss: 0.7908579721504837
test loss: 0.7839506113588001
38
[0.0001]
LR:  None
train loss: 0.2508188567443026
validation loss: 0.787233657537482
test loss: 0.7776788858280449
39
[0.0001]
LR:  None
train loss: 0.2502004181807307
validation loss: 0.785666997908354
test loss: 0.77677678345127
40
[0.0001]
LR:  None
train loss: 0.24937483590798007
validation loss: 0.785110865801828
test loss: 0.7738471910148986
41
[0.0001]
LR:  None
train loss: 0.24890590347475394
validation loss: 0.7851942195825615
test loss: 0.7733321289699355
42
[0.0001]
LR:  None
train loss: 0.248469383305423
validation loss: 0.7825365568635557
test loss: 0.7724549084984003
43
[0.0001]
LR:  None
train loss: 0.2474183447446039
validation loss: 0.7802539848209982
test loss: 0.7700843846463421
44
[0.0001]
LR:  None
train loss: 0.24735234952688326
validation loss: 0.7783300585444015
test loss: 0.7712477425624373
45
[0.0001]
LR:  None
train loss: 0.24632036263340093
validation loss: 0.7785333452633793
test loss: 0.7700884291056196
46
[0.0001]
LR:  None
train loss: 0.2457099988781121
validation loss: 0.7766504341506624
test loss: 0.7665538641254972
47
[0.0001]
LR:  None
train loss: 0.24478523257960322
validation loss: 0.7750057410993164
test loss: 0.7647581815231367
48
[0.0001]
LR:  None
train loss: 0.24424190630775283
validation loss: 0.7724483354820422
test loss: 0.7650610989793868
49
[0.0001]
LR:  None
train loss: 0.2436172226305048
validation loss: 0.7720703555318599
test loss: 0.7623645450072286
50
[0.0001]
LR:  None
train loss: 0.2431100296121968
validation loss: 0.770973929387517
test loss: 0.7626348075631727
51
[0.0001]
LR:  None
train loss: 0.2427296731514842
validation loss: 0.7678152735711837
test loss: 0.7594417026837266
52
[0.0001]
LR:  None
train loss: 0.24219112310609406
validation loss: 0.7681355266863987
test loss: 0.7627750612331843
53
[0.0001]
LR:  None
train loss: 0.24140816043145666
validation loss: 0.7682801733905269
test loss: 0.7617947120655283
54
[0.0001]
LR:  None
train loss: 0.24090394364791434
validation loss: 0.7654961146287209
test loss: 0.754766582101295
55
[0.0001]
LR:  None
train loss: 0.24057710562489296
validation loss: 0.7652314296823636
test loss: 0.7571093501468568
56
[0.0001]
LR:  None
train loss: 0.24018969452991445
validation loss: 0.7626491878129872
test loss: 0.7550690687032509
57
[0.0001]
LR:  None
train loss: 0.2395873437171009
validation loss: 0.7629454269159968
test loss: 0.7544533744585159
58
[0.0001]
LR:  None
train loss: 0.23950526427078708
validation loss: 0.7624182033124868
test loss: 0.7529270774623784
59
[0.0001]
LR:  None
train loss: 0.23882562328145424
validation loss: 0.7623665518883269
test loss: 0.7532252986005229
60
[0.0001]
LR:  None
train loss: 0.23834392328131845
validation loss: 0.762290996732763
test loss: 0.7520859534090083
61
[0.0001]
LR:  None
train loss: 0.23821564564747025
validation loss: 0.7598167292391393
test loss: 0.7533727645023672
62
[0.0001]
LR:  None
train loss: 0.23772571825212208
validation loss: 0.7589081522174983
test loss: 0.754036929814818
63
[0.0001]
LR:  None
train loss: 0.23733257657333548
validation loss: 0.7601669227636778
test loss: 0.7506284968474756
64
[0.0001]
LR:  None
train loss: 0.23737025154384137
validation loss: 0.7607490667740854
test loss: 0.7524948789687997
65
[0.0001]
LR:  None
train loss: 0.2363614861606508
validation loss: 0.7595706689291041
test loss: 0.7502198949311859
66
[0.0001]
LR:  None
train loss: 0.23618724503663027
validation loss: 0.7581510369704751
test loss: 0.7511599274329727
67
[0.0001]
LR:  None
train loss: 0.235722160780161
validation loss: 0.7589367208847342
test loss: 0.7513882830351267
68
[0.0001]
LR:  None
train loss: 0.23564839731600873
validation loss: 0.7553527636004078
test loss: 0.7470816406517193
69
[0.0001]
LR:  None
train loss: 0.23512245008562782
validation loss: 0.7564886610052008
test loss: 0.7499033207775572
70
[0.0001]
LR:  None
train loss: 0.2348011283008811
validation loss: 0.7557719921264104
test loss: 0.7497217207349417
71
[0.0001]
LR:  None
train loss: 0.23470369527179327
validation loss: 0.7570146709777319
test loss: 0.7494885866156956
72
[0.0001]
LR:  None
train loss: 0.23459442314968973
validation loss: 0.7561189638009249
test loss: 0.7490139324586771
73
[0.0001]
LR:  None
train loss: 0.23409520749782603
validation loss: 0.7544832669622425
test loss: 0.7508678691061604
74
[0.0001]
LR:  None
train loss: 0.2337900816658208
validation loss: 0.7546726788267631
test loss: 0.7454622813537831
75
[0.0001]
LR:  None
train loss: 0.23328135472738984
validation loss: 0.7561817852781078
test loss: 0.747073279159678
76
[0.0001]
LR:  None
train loss: 0.23325422484907468
validation loss: 0.7560873569716479
test loss: 0.750099541261817
77
[0.0001]
LR:  None
train loss: 0.2323559562569165
validation loss: 0.7538989315171121
test loss: 0.7479515164234437
78
[0.0001]
LR:  None
train loss: 0.23239918768578605
validation loss: 0.753649736828335
test loss: 0.7419498429390872
79
[0.0001]
LR:  None
train loss: 0.23187042588830534
validation loss: 0.7543133932294458
test loss: 0.7479365974315147
80
[0.0001]
LR:  None
train loss: 0.2316059552621779
validation loss: 0.7542087783689764
test loss: 0.7479805880576686
81
[0.0001]
LR:  None
train loss: 0.2317576070523996
validation loss: 0.7525869749898318
test loss: 0.7448574533032623
82
[0.0001]
LR:  None
train loss: 0.2310743659949592
validation loss: 0.7534438665324387
test loss: 0.7449662202679432
83
[0.0001]
LR:  None
train loss: 0.23058183300736307
validation loss: 0.7546847471315472
test loss: 0.7474112294986147
84
[0.0001]
LR:  None
train loss: 0.2307492796308299
validation loss: 0.7531525946328419
test loss: 0.7441978587104203
85
[0.0001]
LR:  None
train loss: 0.2303058268380619
validation loss: 0.7512248892154254
test loss: 0.7430749702873367
86
[0.0001]
LR:  None
train loss: 0.23013890663080563
validation loss: 0.7532159555885263
test loss: 0.7462736282307775
87
[0.0001]
LR:  None
train loss: 0.22957159937575664
validation loss: 0.7540003786203808
test loss: 0.7414453558821508
88
[0.0001]
LR:  None
train loss: 0.22948977740148513
validation loss: 0.7517341819769774
test loss: 0.7423561459850072
89
[0.0001]
LR:  None
train loss: 0.22922321991621222
validation loss: 0.7525779123158168
test loss: 0.7447162138216945
90
[0.0001]
LR:  None
train loss: 0.22891899631370383
validation loss: 0.75218928362583
test loss: 0.7447931432579751
91
[0.0001]
LR:  None
train loss: 0.2288212515667239
validation loss: 0.752324581860338
test loss: 0.742727678601935
92
[0.0001]
LR:  None
train loss: 0.22826232552931036
validation loss: 0.7509580671848759
test loss: 0.7431510494887774
93
[0.0001]
LR:  None
train loss: 0.22799378454045052
validation loss: 0.7507603722612028
test loss: 0.7409988421485547
94
[0.0001]
LR:  None
train loss: 0.22827730317597514
validation loss: 0.7517522031548527
test loss: 0.7438633199612452
95
[0.0001]
LR:  None
train loss: 0.2273621463332234
validation loss: 0.7502979404928478
test loss: 0.7417429413857716
96
[0.0001]
LR:  None
train loss: 0.2276255337808364
validation loss: 0.7512513574880059
test loss: 0.7415859483437779
97
[0.0001]
LR:  None
train loss: 0.2270692465076519
validation loss: 0.7508155880832139
test loss: 0.7426548424489609
98
[0.0001]
LR:  None
train loss: 0.2270812475228343
validation loss: 0.7514599428201821
test loss: 0.7449997319122692
99
[0.0001]
LR:  None
train loss: 0.2265626364773848
validation loss: 0.7514799023510252
test loss: 0.7418201139657257
100
[0.0001]
LR:  None
train loss: 0.22630161064503104
validation loss: 0.7510620534444465
test loss: 0.7449746136221116
101
[0.0001]
LR:  None
train loss: 0.22606102343639747
validation loss: 0.7514053594740467
test loss: 0.7446708493235189
102
[0.0001]
LR:  None
train loss: 0.22582224004764387
validation loss: 0.7502712335389751
test loss: 0.7399987175389307
103
[0.0001]
LR:  None
train loss: 0.22590549732579682
validation loss: 0.7509581132997406
test loss: 0.7461965426788069
104
[0.0001]
LR:  None
train loss: 0.22544118116246825
validation loss: 0.7492626059990422
test loss: 0.7449832936833366
105
[0.0001]
LR:  None
train loss: 0.2250366057913739
validation loss: 0.7508238466779226
test loss: 0.7469218693024227
106
[0.0001]
LR:  None
train loss: 0.22496805364060093
validation loss: 0.7506334366637084
test loss: 0.7403534295338031
107
[0.0001]
LR:  None
train loss: 0.22502656159277365
validation loss: 0.7503540319844075
test loss: 0.7476824892089006
108
[0.0001]
LR:  None
train loss: 0.22438837118039048
validation loss: 0.7500090218296
test loss: 0.7432441379978594
109
[0.0001]
LR:  None
train loss: 0.22409226452499298
validation loss: 0.7500849459634993
test loss: 0.7419602717302023
110
[0.0001]
LR:  None
train loss: 0.22361554313796847
validation loss: 0.7495417066532438
test loss: 0.7411023338714013
111
[0.0001]
LR:  None
train loss: 0.2238220008080963
validation loss: 0.7490823406872222
test loss: 0.7386798533393631
112
[0.0001]
LR:  None
train loss: 0.2233476612276875
validation loss: 0.7503507325868245
test loss: 0.7389849573172317
113
[0.0001]
LR:  None
train loss: 0.22292124411630135
validation loss: 0.750723030133335
test loss: 0.7439215731248663
114
[0.0001]
LR:  None
train loss: 0.22330271570992521
validation loss: 0.7489824061267218
test loss: 0.7390975656238619
115
[0.0001]
LR:  None
train loss: 0.22274795716671517
validation loss: 0.7505550263766828
test loss: 0.7439786772847415
116
[0.0001]
LR:  None
train loss: 0.22250021359285205
validation loss: 0.7490319773891518
test loss: 0.7402199995357329
117
[0.0001]
LR:  None
train loss: 0.2224180844100105
validation loss: 0.7484989580073489
test loss: 0.737629826393297
118
[0.0001]
LR:  None
train loss: 0.22202227754041123
validation loss: 0.7493153396728386
test loss: 0.7382336249014371
119
[0.0001]
LR:  None
train loss: 0.22212606432037232
validation loss: 0.7487838865345364
test loss: 0.7422218610220508
120
[0.0001]
LR:  None
train loss: 0.2217441431717401
validation loss: 0.7491981003614824
test loss: 0.7411767301743512
121
[0.0001]
LR:  None
train loss: 0.22171362432851674
validation loss: 0.7516830388517947
test loss: 0.740756755709157
122
[0.0001]
LR:  None
train loss: 0.22128102156812568
validation loss: 0.7502890602441765
test loss: 0.7398144121623577
123
[0.0001]
LR:  None
train loss: 0.2209960465611186
validation loss: 0.7490588195858949
test loss: 0.7408010927885049
124
[0.0001]
LR:  None
train loss: 0.22094875518636498
validation loss: 0.7503813094007161
test loss: 0.7395338987016329
125
[0.0001]
LR:  None
train loss: 0.2210425957877925
validation loss: 0.7499345379390259
test loss: 0.7420056634514279
126
[0.0001]
LR:  None
train loss: 0.22071847875847542
validation loss: 0.7495068709939737
test loss: 0.7417933481850068
127
[0.0001]
LR:  None
train loss: 0.22072552835199144
validation loss: 0.749470407938462
test loss: 0.7421248239462986
128
[0.0001]
LR:  None
train loss: 0.2200710968950493
validation loss: 0.7496702791349242
test loss: 0.7409761232889469
129
[0.0001]
LR:  None
train loss: 0.22021018664250938
validation loss: 0.7495810749251213
test loss: 0.7413918590664089
130
[0.0001]
LR:  None
train loss: 0.2195274039171283
validation loss: 0.7506739623455357
test loss: 0.7441872523649364
131
[0.0001]
LR:  None
train loss: 0.21950217388376042
validation loss: 0.7495209386610028
test loss: 0.7419996054020944
132
[0.0001]
LR:  None
train loss: 0.21934129681749864
validation loss: 0.7503787086163657
test loss: 0.7408193232880657
133
[0.0001]
LR:  None
train loss: 0.2192161817451845
validation loss: 0.7497425449031927
test loss: 0.7393867732165007
134
[0.0001]
LR:  None
train loss: 0.21914831861581083
validation loss: 0.7492268600965344
test loss: 0.7418124820046424
135
[0.0001]
LR:  None
train loss: 0.21902414564408232
validation loss: 0.7498541016543123
test loss: 0.744998357438226
136
[0.0001]
LR:  None
train loss: 0.21875746190281098
validation loss: 0.7523826456401942
test loss: 0.7414107711127993
137
[0.0001]
LR:  None
train loss: 0.2184631949956094
validation loss: 0.7501823062442984
test loss: 0.7419166363459269
ES epoch: 117
Test data
Skills for tau_11
R^2: 0.9311
Correlation: 0.9671

Skills for tau_12
R^2: 0.7016
Correlation: 0.8409

Skills for tau_13
R^2: 0.7394
Correlation: 0.8653

Skills for tau_22
R^2: 0.7740
Correlation: 0.8842

Skills for tau_23
R^2: 0.6856
Correlation: 0.8336

Skills for tau_33
R^2: 0.6701
Correlation: 0.8420

Validation data
Skills for tau_11
R^2: 0.9274
Correlation: 0.9659

Skills for tau_12
R^2: 0.6937
Correlation: 0.8365

Skills for tau_13
R^2: 0.7427
Correlation: 0.8663

Skills for tau_22
R^2: 0.7690
Correlation: 0.8816

Skills for tau_23
R^2: 0.6849
Correlation: 0.8331

Skills for tau_33
R^2: 0.6616
Correlation: 0.8389

Train data
Skills for tau_11
R^2: 0.9732
Correlation: 0.9868

Skills for tau_12
R^2: 0.8534
Correlation: 0.9239

Skills for tau_13
R^2: 0.6793
Correlation: 0.8282

Skills for tau_22
R^2: 0.8600
Correlation: 0.9294

Skills for tau_23
R^2: 0.7204
Correlation: 0.8504

Skills for tau_33
R^2: 0.2947
Correlation: 0.5706

Train Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 44)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 590200 590400 590600 590800 ... 608000 609000 610000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 15)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 616000 617000 618000 619000 ... 628000 629000 630000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (110160, 6)
input shape should be (110160, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (110160, 12, 3, 3)
Test Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 3)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 1368000 1377000 1386000
Data variables:
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    tau12    (z, y, x, time) float64 ...
    tau13    (z, y, x, time) float64 ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (141312, 6)
input shape should be (141312, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (141312, 12, 3, 3)
Lossweights:
[  315388.0848  1667653.0636  7450675.2636  1101290.1761 10319803.5072  5425051.762 ]
0
[0.01]
LR:  None
train loss: 0.29781539715852473
validation loss: 0.8920884526462611
test loss: 0.8949031389180201
1
[0.001]
LR:  None
train loss: 0.28522344449722326
validation loss: 0.864787960825507
test loss: 0.8667018577629192
2
[0.0001]
LR:  None
train loss: 0.28391679470261355
validation loss: 0.8619789246802247
test loss: 0.8646403436405609
3
[0.0001]
LR:  None
train loss: 0.28333241802514003
validation loss: 0.8601136254803663
test loss: 0.8631763261197332
4
[0.0001]
LR:  None
train loss: 0.28295970904387246
validation loss: 0.8600506614018527
test loss: 0.8625706937784716
5
[0.0001]
LR:  None
train loss: 0.282326912052465
validation loss: 0.8579780385402213
test loss: 0.8603061479071462
6
[0.0001]
LR:  None
train loss: 0.28175026668985775
validation loss: 0.8577148233057322
test loss: 0.8602235296041842
7
[0.0001]
LR:  None
train loss: 0.28123967831728824
validation loss: 0.8558806971132245
test loss: 0.8576205873820799
8
[0.0001]
LR:  None
train loss: 0.2805774322195344
validation loss: 0.8550823690488932
test loss: 0.8572739672555081
9
[0.0001]
LR:  None
train loss: 0.2799479515039757
validation loss: 0.8536021988795859
test loss: 0.8553331711428395
10
[0.0001]
LR:  None
train loss: 0.2793563130358339
validation loss: 0.8524839595128132
test loss: 0.8547523481073843
11
[0.0001]
LR:  None
train loss: 0.27877813543560487
validation loss: 0.8504683760369697
test loss: 0.8528267171742737
12
[0.0001]
LR:  None
train loss: 0.2780389863623727
validation loss: 0.8506681586916254
test loss: 0.8532735040995175
13
[0.0001]
LR:  None
train loss: 0.2774795796115837
validation loss: 0.8487919436824057
test loss: 0.8512916878650484
14
[0.0001]
LR:  None
train loss: 0.27678046424363684
validation loss: 0.8461738994270381
test loss: 0.8492025965515531
15
[0.0001]
LR:  None
train loss: 0.27605631503350514
validation loss: 0.845749882385254
test loss: 0.8485601314018756
16
[0.0001]
LR:  None
train loss: 0.2753582050169494
validation loss: 0.8440010589111601
test loss: 0.8469602255996121
17
[0.0001]
LR:  None
train loss: 0.2748831360065393
validation loss: 0.8427989543194081
test loss: 0.8454744272002961
18
[0.0001]
LR:  None
train loss: 0.2742938412169466
validation loss: 0.841716971017325
test loss: 0.8442991526462298
19
[0.0001]
LR:  None
train loss: 0.2736438379024134
validation loss: 0.8407852109316356
test loss: 0.8431494365356388
20
[0.0001]
LR:  None
train loss: 0.2729657219538011
validation loss: 0.8401959955253174
test loss: 0.8433323555354011
21
[0.0001]
LR:  None
train loss: 0.2723349183518533
validation loss: 0.838374650810575
test loss: 0.8405432730493231
22
[0.0001]
LR:  None
train loss: 0.27188922630538426
validation loss: 0.8374684353496242
test loss: 0.8392942707743031
23
[0.0001]
LR:  None
train loss: 0.2712201890100417
validation loss: 0.8362089274086029
test loss: 0.8384640436358963
24
[0.0001]
LR:  None
train loss: 0.27062791488158955
validation loss: 0.8329504349304475
test loss: 0.8354437521528905
25
[0.0001]
LR:  None
train loss: 0.2701194873730514
validation loss: 0.8335006798005837
test loss: 0.8351068583558581
26
[0.0001]
LR:  None
train loss: 0.26951012442378464
validation loss: 0.831432323621475
test loss: 0.8340754737970772
27
[0.0001]
LR:  None
train loss: 0.26919491090375874
validation loss: 0.8321282973431041
test loss: 0.8339913449973287
28
[0.0001]
LR:  None
train loss: 0.26865936181349065
validation loss: 0.8296997097649095
test loss: 0.8317743398819205
29
[0.0001]
LR:  None
train loss: 0.2679019418578616
validation loss: 0.8280952585222939
test loss: 0.8304766752873307
30
[0.0001]
LR:  None
train loss: 0.26739553439519803
validation loss: 0.8265096979362064
test loss: 0.828885529766054
31
[0.0001]
LR:  None
train loss: 0.2668879438083298
validation loss: 0.8253784039759006
test loss: 0.8276882193134065
32
[0.0001]
LR:  None
train loss: 0.266424575479995
validation loss: 0.8245934267672942
test loss: 0.8271689148910599
33
[0.0001]
LR:  None
train loss: 0.2658451736288456
validation loss: 0.8237561557777905
test loss: 0.826287034343582
34
[0.0001]
LR:  None
train loss: 0.26559310947251674
validation loss: 0.8216522737497503
test loss: 0.8241660713108058
35
[0.0001]
LR:  None
train loss: 0.2650181669358992
validation loss: 0.8207104030730238
test loss: 0.8232335260322939
36
[0.0001]
LR:  None
train loss: 0.2644686063814216
validation loss: 0.8208394578123028
test loss: 0.8240908680706853
37
[0.0001]
LR:  None
train loss: 0.26390213357786546
validation loss: 0.8185404737471152
test loss: 0.8206126063885307
38
[0.0001]
LR:  None
train loss: 0.26354604086824795
validation loss: 0.8174357106804425
test loss: 0.8191896201217745
39
[0.0001]
LR:  None
train loss: 0.2629476608499737
validation loss: 0.8169711304728636
test loss: 0.8193549375645088
40
[0.0001]
LR:  None
train loss: 0.2626328052071148
validation loss: 0.814834212959762
test loss: 0.8179085347605688
41
[0.0001]
LR:  None
train loss: 0.262302772880735
validation loss: 0.8147575032554805
test loss: 0.817437066813809
42
[0.0001]
LR:  None
train loss: 0.2618385641987353
validation loss: 0.8138330978972114
test loss: 0.8165321909154734
43
[0.0001]
LR:  None
train loss: 0.2614349656896204
validation loss: 0.8113147015242134
test loss: 0.8135695516433488
44
[0.0001]
LR:  None
train loss: 0.261102966091145
validation loss: 0.8117332409349708
test loss: 0.8137395284810515
45
[0.0001]
LR:  None
train loss: 0.2605638571999412
validation loss: 0.8106963870703695
test loss: 0.8137582511966424
46
[0.0001]
LR:  None
train loss: 0.2603054553152112
validation loss: 0.8102761568279557
test loss: 0.8126077222087422
47
[0.0001]
LR:  None
train loss: 0.2599648133321563
validation loss: 0.8091856518742666
test loss: 0.8122350216068056
48
[0.0001]
LR:  None
train loss: 0.25956575856161435
validation loss: 0.8074913198938473
test loss: 0.8107980936257263
49
[0.0001]
LR:  None
train loss: 0.25920776021856295
validation loss: 0.8077333466771218
test loss: 0.8108773122257804
50
[0.0001]
LR:  None
train loss: 0.25874383190863287
validation loss: 0.8080967703301744
test loss: 0.8113062977977255
51
[0.0001]
LR:  None
train loss: 0.2584775199922769
validation loss: 0.805098665821984
test loss: 0.8080486969047393
52
[0.0001]
LR:  None
train loss: 0.25800404386508324
validation loss: 0.8054291059305052
test loss: 0.8086292906247897
53
[0.0001]
LR:  None
train loss: 0.2578822348072037
validation loss: 0.8047048796912629
test loss: 0.8088896385573067
54
[0.0001]
LR:  None
train loss: 0.25752398190737835
validation loss: 0.8045511133082813
test loss: 0.8081942686249997
55
[0.0001]
LR:  None
train loss: 0.2571542066675044
validation loss: 0.8037519820040427
test loss: 0.806456905356735
56
[0.0001]
LR:  None
train loss: 0.25680155590477804
validation loss: 0.8020015338998159
test loss: 0.8054854946783209
57
[0.0001]
LR:  None
train loss: 0.2566552810273054
validation loss: 0.8038523518060823
test loss: 0.8065787860719901
58
[0.0001]
LR:  None
train loss: 0.2561273179150284
validation loss: 0.8015652564844722
test loss: 0.8048887410038436
59
[0.0001]
LR:  None
train loss: 0.2559644881309694
validation loss: 0.8016444849680383
test loss: 0.8051527162073845
60
[0.0001]
LR:  None
train loss: 0.25571753801547864
validation loss: 0.799894674728995
test loss: 0.8024965482825281
61
[0.0001]
LR:  None
train loss: 0.2554040351337941
validation loss: 0.8014369704119423
test loss: 0.8045062258718272
62
[0.0001]
LR:  None
train loss: 0.2552700709551052
validation loss: 0.8003679088413016
test loss: 0.8029826453904542
63
[0.0001]
LR:  None
train loss: 0.25471351970453326
validation loss: 0.7994012773679091
test loss: 0.8023000229749762
64
[0.0001]
LR:  None
train loss: 0.25456115862272943
validation loss: 0.7984123908148227
test loss: 0.8006693410361069
65
[0.0001]
LR:  None
train loss: 0.2543892224505489
validation loss: 0.7985205315351408
test loss: 0.8018480732276068
66
[0.0001]
LR:  None
train loss: 0.2539343290959869
validation loss: 0.7981067241977493
test loss: 0.8013136286787372
67
[0.0001]
LR:  None
train loss: 0.25359005878696206
validation loss: 0.7975218576537033
test loss: 0.8010654972005742
68
[0.0001]
LR:  None
train loss: 0.25338629732542145
validation loss: 0.7964025169471676
test loss: 0.7995955711408241
69
[0.0001]
LR:  None
train loss: 0.25291289051092686
validation loss: 0.7948731291365156
test loss: 0.7977111327415972
70
[0.0001]
LR:  None
train loss: 0.2525494057532387
validation loss: 0.7958606404749468
test loss: 0.7992791674886752
71
[0.0001]
LR:  None
train loss: 0.25248285190238035
validation loss: 0.7932197236730655
test loss: 0.7965974177122698
72
[0.0001]
LR:  None
train loss: 0.2521479135919698
validation loss: 0.7948855586341718
test loss: 0.7977179398543862
73
[0.0001]
LR:  None
train loss: 0.2519240821928832
validation loss: 0.7941974121186516
test loss: 0.7978692034622819
74
[0.0001]
LR:  None
train loss: 0.25148046186653417
validation loss: 0.7935128540022933
test loss: 0.7961248530933329
75
[0.0001]
LR:  None
train loss: 0.25132099057958535
validation loss: 0.7931337045111365
test loss: 0.7964994694913096
76
[0.0001]
LR:  None
train loss: 0.2509226388229775
validation loss: 0.7917865664680632
test loss: 0.7950182757722103
77
[0.0001]
LR:  None
train loss: 0.2510225536289395
validation loss: 0.7928333300937684
test loss: 0.7958306201065751
78
[0.0001]
LR:  None
train loss: 0.25042098211393715
validation loss: 0.7909603199886515
test loss: 0.7940068566887505
79
[0.0001]
LR:  None
train loss: 0.25020791441320445
validation loss: 0.7902148688853607
test loss: 0.793792448940232
80
[0.0001]
LR:  None
train loss: 0.24983625730780795
validation loss: 0.7903300324958747
test loss: 0.79355142716784
81
[0.0001]
LR:  None
train loss: 0.24951613513608434
validation loss: 0.7898282038469996
test loss: 0.7933742785950015
82
[0.0001]
LR:  None
train loss: 0.24933938398364203
validation loss: 0.7902624487025386
test loss: 0.7939262598050247
83
[0.0001]
LR:  None
train loss: 0.24909641916626327
validation loss: 0.7889761438577173
test loss: 0.7917346555285504
84
[0.0001]
LR:  None
train loss: 0.24874858275790648
validation loss: 0.7904305415189395
test loss: 0.7943529410295475
85
[0.0001]
LR:  None
train loss: 0.24834531433968804
validation loss: 0.7887639393909367
test loss: 0.7914881429045488
86
[0.0001]
LR:  None
train loss: 0.24807044506438036
validation loss: 0.7873176075706216
test loss: 0.7904968340758348
87
[0.0001]
LR:  None
train loss: 0.2477793304224585
validation loss: 0.7875740750930923
test loss: 0.7907889596612714
88
[0.0001]
LR:  None
train loss: 0.2475533511066297
validation loss: 0.7875064916964112
test loss: 0.7909316102392548
89
[0.0001]
LR:  None
train loss: 0.24720629293005286
validation loss: 0.7880797539438891
test loss: 0.7913961132079359
90
[0.0001]
LR:  None
train loss: 0.24713002507913331
validation loss: 0.7857511753748854
test loss: 0.787959538208744
91
[0.0001]
LR:  None
train loss: 0.2467091144469979
validation loss: 0.7857623908194996
test loss: 0.7895040265796548
92
[0.0001]
LR:  None
train loss: 0.24646512396157297
validation loss: 0.7855048520274847
test loss: 0.788955281361091
93
[0.0001]
LR:  None
train loss: 0.2459134753492816
validation loss: 0.7835803004944278
test loss: 0.7863006700481264
94
[0.0001]
LR:  None
train loss: 0.24556516280246565
validation loss: 0.78339717675261
test loss: 0.7862723268533565
95
[0.0001]
LR:  None
train loss: 0.24530053741240235
validation loss: 0.7827439138730301
test loss: 0.785972217827488
96
[0.0001]
LR:  None
train loss: 0.24480050209602378
validation loss: 0.7813995600099076
test loss: 0.7845120065801223
97
[0.0001]
LR:  None
train loss: 0.2444670597665953
validation loss: 0.7811286339679083
test loss: 0.7841750658645887
98
[0.0001]
LR:  None
train loss: 0.2441843959065433
validation loss: 0.7800373064954927
test loss: 0.7830926412577986
99
[0.0001]
LR:  None
train loss: 0.24377083948730943
validation loss: 0.7799817184779417
test loss: 0.7831672030496496
100
[0.0001]
LR:  None
train loss: 0.24331080524833
validation loss: 0.7778700428873785
test loss: 0.7810781402316724
101
[0.0001]
LR:  None
train loss: 0.24287668762783235
validation loss: 0.7756967289536834
test loss: 0.779180988292244
102
[0.0001]
LR:  None
train loss: 0.2420897286227145
validation loss: 0.7747637540924102
test loss: 0.7772131354457629
103
[0.0001]
LR:  None
train loss: 0.24169238492679224
validation loss: 0.7750184795099448
test loss: 0.7772807372084583
104
[0.0001]
LR:  None
train loss: 0.24095232153099028
validation loss: 0.7723696923168938
test loss: 0.7750145912621381
105
[0.0001]
LR:  None
train loss: 0.24061360155249292
validation loss: 0.7710939039286859
test loss: 0.7741598159919093
106
[0.0001]
LR:  None
train loss: 0.24017352496340627
validation loss: 0.7695002444516424
test loss: 0.7721374935732018
107
[0.0001]
LR:  None
train loss: 0.23975008313311297
validation loss: 0.7688320781300171
test loss: 0.7717809386885005
108
[0.0001]
LR:  None
train loss: 0.2391180083241542
validation loss: 0.7680559401407379
test loss: 0.7703573749201175
109
[0.0001]
LR:  None
train loss: 0.2389253054273561
validation loss: 0.7659289618182893
test loss: 0.768159544647313
110
[0.0001]
LR:  None
train loss: 0.2383636786406991
validation loss: 0.7654082099127532
test loss: 0.7684950611014267
111
[0.0001]
LR:  None
train loss: 0.23846578000477606
validation loss: 0.7671847569715917
test loss: 0.7697205289830051
112
[0.0001]
LR:  None
train loss: 0.23765515938389367
validation loss: 0.763999873866129
test loss: 0.7659584862689813
113
[0.0001]
LR:  None
train loss: 0.23721216384283206
validation loss: 0.7624931906782898
test loss: 0.7655212112441753
114
[0.0001]
LR:  None
train loss: 0.23699432814517257
validation loss: 0.762466245063408
test loss: 0.7663303380616882
115
[0.0001]
LR:  None
train loss: 0.2366948159531031
validation loss: 0.7619557583851052
test loss: 0.7646260186521733
116
[0.0001]
LR:  None
train loss: 0.23695803938392346
validation loss: 0.7619087425224804
test loss: 0.7647741046109001
117
[0.0001]
LR:  None
train loss: 0.2361238749928479
validation loss: 0.7592666127196336
test loss: 0.7620460217862692
118
[0.0001]
LR:  None
train loss: 0.23573538188761212
validation loss: 0.7603020768025778
test loss: 0.7629025459969994
119
[0.0001]
LR:  None
train loss: 0.23531101202385465
validation loss: 0.7587434363607788
test loss: 0.761472307178506
120
[0.0001]
LR:  None
train loss: 0.23527063841651183
validation loss: 0.7611900532211973
test loss: 0.7637807359722796
121
[0.0001]
LR:  None
train loss: 0.23487113297399312
validation loss: 0.7600709878778295
test loss: 0.7622390259495191
122
[0.0001]
LR:  None
train loss: 0.2348648020821733
validation loss: 0.7582962998555606
test loss: 0.7613540289429872
123
[0.0001]
LR:  None
train loss: 0.23450227101017146
validation loss: 0.7575497211651036
test loss: 0.7607208056800093
124
[0.0001]
LR:  None
train loss: 0.23408601934668638
validation loss: 0.7579463212462828
test loss: 0.760889637376964
125
[0.0001]
LR:  None
train loss: 0.2337098285211757
validation loss: 0.7565807242584647
test loss: 0.7595538241199049
126
[0.0001]
LR:  None
train loss: 0.2335048038394623
validation loss: 0.7571409942122387
test loss: 0.7593999030802001
127
[0.0001]
LR:  None
train loss: 0.23355413658129773
validation loss: 0.7579798012386021
test loss: 0.7605422378719092
128
[0.0001]
LR:  None
train loss: 0.23306062959334042
validation loss: 0.7564252513016412
test loss: 0.7596155705589133
129
[0.0001]
LR:  None
train loss: 0.23276092704695742
validation loss: 0.7557096716092427
test loss: 0.758018577116388
130
[0.0001]
LR:  None
train loss: 0.23280922732584072
validation loss: 0.7555274041966419
test loss: 0.7581741562530259
131
[0.0001]
LR:  None
train loss: 0.23235297486124595
validation loss: 0.7547355390567722
test loss: 0.7578404587541024
132
[0.0001]
LR:  None
train loss: 0.23220497371437498
validation loss: 0.7563058333692029
test loss: 0.7596988263472179
133
[0.0001]
LR:  None
train loss: 0.2319111824624801
validation loss: 0.7551382296841113
test loss: 0.7575059969185046
134
[0.0001]
LR:  None
train loss: 0.23184673222481955
validation loss: 0.75520670605935
test loss: 0.7570735897525194
135
[0.0001]
LR:  None
train loss: 0.23120358867283294
validation loss: 0.7536198078636023
test loss: 0.7563552530476223
136
[0.0001]
LR:  None
train loss: 0.23112734127861903
validation loss: 0.7532758126545093
test loss: 0.756634605680263
137
[0.0001]
LR:  None
train loss: 0.23084882923215289
validation loss: 0.7540407871181901
test loss: 0.7559020933737665
138
[0.0001]
LR:  None
train loss: 0.23063165088999535
validation loss: 0.7541593575159928
test loss: 0.7566643514269096
139
[0.0001]
LR:  None
train loss: 0.23048120481592757
validation loss: 0.7527141663453952
test loss: 0.7552908840929474
140
[0.0001]
LR:  None
train loss: 0.23036798942399186
validation loss: 0.7518405170443327
test loss: 0.7544732510275124
141
[0.0001]
LR:  None
train loss: 0.23007081729305043
validation loss: 0.7527540285606067
test loss: 0.7547924430053479
142
[0.0001]
LR:  None
train loss: 0.23002006701855798
validation loss: 0.7531109301866626
test loss: 0.7554385209064953
143
[0.0001]
LR:  None
train loss: 0.2297754218281816
validation loss: 0.752573517788338
test loss: 0.7556946558248494
144
[0.0001]
LR:  None
train loss: 0.22977230940748056
validation loss: 0.7529306147917233
test loss: 0.7562421983735342
145
[0.0001]
LR:  None
train loss: 0.22920278659310148
validation loss: 0.7515969368269791
test loss: 0.7541462420630959
146
[0.0001]
LR:  None
train loss: 0.2289606670147967
validation loss: 0.7520964476076326
test loss: 0.7539507314269978
147
[0.0001]
LR:  None
train loss: 0.22900775343619276
validation loss: 0.7506803514655239
test loss: 0.7528123414091059
148
[0.0001]
LR:  None
train loss: 0.22864514345242404
validation loss: 0.7514612366255896
test loss: 0.7536437071740916
149
[0.0001]
LR:  None
train loss: 0.2284715745653695
validation loss: 0.7522154626949875
test loss: 0.7545582162275054
150
[0.0001]
LR:  None
train loss: 0.2283019448226233
validation loss: 0.751977861373666
test loss: 0.7542498878616283
151
[0.0001]
LR:  None
train loss: 0.22813233606346706
validation loss: 0.7519351807080709
test loss: 0.754494233724639
152
[0.0001]
LR:  None
train loss: 0.22817564848775018
validation loss: 0.751159746906004
test loss: 0.7530402546721313
153
[0.0001]
LR:  None
train loss: 0.22800288710978536
validation loss: 0.7514743238313453
test loss: 0.7547583458643564
154
[0.0001]
LR:  None
train loss: 0.22757562373909831
validation loss: 0.7512898005050473
test loss: 0.7539920076752169
155
[0.0001]
LR:  None
train loss: 0.22745194703035296
validation loss: 0.7511459059954696
test loss: 0.753674509781418
156
[0.0001]
LR:  None
train loss: 0.22719904171948094
validation loss: 0.7510881208868362
test loss: 0.7535160232914422
157
[0.0001]
LR:  None
train loss: 0.22714185749136392
validation loss: 0.7531347026288302
test loss: 0.7557977033407386
158
[0.0001]
LR:  None
train loss: 0.2268652857369437
validation loss: 0.7504762544819167
test loss: 0.7524944900641338
159
[0.0001]
LR:  None
train loss: 0.22688400896344058
validation loss: 0.749364774211994
test loss: 0.7523096718906044
160
[0.0001]
LR:  None
train loss: 0.2268239799748079
validation loss: 0.7519219751652819
test loss: 0.7546236052389769
161
[0.0001]
LR:  None
train loss: 0.22649344623670972
validation loss: 0.7515361987002535
test loss: 0.7542551494787246
162
[0.0001]
LR:  None
train loss: 0.22630630595567833
validation loss: 0.7507703469240339
test loss: 0.7537215536128943
163
[0.0001]
LR:  None
train loss: 0.22612528871137924
validation loss: 0.7503417442350653
test loss: 0.7526197778554904
164
[0.0001]
LR:  None
train loss: 0.22611628017489752
validation loss: 0.7519333192274438
test loss: 0.7540965689040462
165
[0.0001]
LR:  None
train loss: 0.22580231417532146
validation loss: 0.7504048100889018
test loss: 0.7528385317729112
166
[0.0001]
LR:  None
train loss: 0.22573109182673984
validation loss: 0.7497695281586757
test loss: 0.7534524836354296
167
[0.0001]
LR:  None
train loss: 0.22562311223165493
validation loss: 0.7502952581333788
test loss: 0.7525797795297813
168
[0.0001]
LR:  None
train loss: 0.22537599615706738
validation loss: 0.7508106577322018
test loss: 0.7543036604963623
169
[0.0001]
LR:  None
train loss: 0.22531204370598024
validation loss: 0.7511022931760335
test loss: 0.7533194634002541
170
[0.0001]
LR:  None
train loss: 0.22513235573644114
validation loss: 0.7505120080846526
test loss: 0.7528248462202011
171
[0.0001]
LR:  None
train loss: 0.22514705862673384
validation loss: 0.7497577100003939
test loss: 0.7531440435857715
172
[0.0001]
LR:  None
train loss: 0.2248961368596582
validation loss: 0.7496314798850712
test loss: 0.7520594853401836
173
[0.0001]
LR:  None
train loss: 0.22480194632916486
validation loss: 0.750509300742356
test loss: 0.752722300502613
174
[0.0001]
LR:  None
train loss: 0.22470223253932112
validation loss: 0.7503557986678645
test loss: 0.7531102248308872
175
[0.0001]
LR:  None
train loss: 0.2246308228421255
validation loss: 0.7510028171312039
test loss: 0.7541071334636091
176
[0.0001]
LR:  None
train loss: 0.22440161976971768
validation loss: 0.7511613476992347
test loss: 0.7545061529307314
177
[0.0001]
LR:  None
train loss: 0.22397764729065822
validation loss: 0.7495505227599328
test loss: 0.7516468175738193
178
[0.0001]
LR:  None
train loss: 0.22403142554386643
validation loss: 0.7503920682222219
test loss: 0.7529286864720237
179
[0.0001]
LR:  None
train loss: 0.22390470859728423
validation loss: 0.7502041602551339
test loss: 0.7538022900543067
ES epoch: 159
Test data
Skills for tau_11
R^2: 0.9316
Correlation: 0.9671

Skills for tau_12
R^2: 0.6994
Correlation: 0.8402

Skills for tau_13
R^2: 0.7474
Correlation: 0.8676

Skills for tau_22
R^2: 0.7809
Correlation: 0.8873

Skills for tau_23
R^2: 0.6821
Correlation: 0.8300

Skills for tau_33
R^2: 0.6685
Correlation: 0.8411

Validation data
Skills for tau_11
R^2: 0.9306
Correlation: 0.9669

Skills for tau_12
R^2: 0.6910
Correlation: 0.8355

Skills for tau_13
R^2: 0.7447
Correlation: 0.8662

Skills for tau_22
R^2: 0.7774
Correlation: 0.8852

Skills for tau_23
R^2: 0.6793
Correlation: 0.8289

Skills for tau_33
R^2: 0.6674
Correlation: 0.8424

Train data
Skills for tau_11
R^2: 0.9726
Correlation: 0.9865

Skills for tau_12
R^2: 0.8462
Correlation: 0.9203

Skills for tau_13
R^2: 0.7039
Correlation: 0.8407

Skills for tau_22
R^2: 0.8444
Correlation: 0.9215

Skills for tau_23
R^2: 0.7247
Correlation: 0.8529

Skills for tau_33
R^2: 0.3125
Correlation: 0.5791

Train Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 44)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 590200 590400 590600 590800 ... 608000 609000 610000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 15)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 616000 617000 618000 619000 ... 628000 629000 630000
Data variables: (12/14)
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    ...       ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    ub       (z, y, x, time) float64 ...
    vb       (z, y, x, time) float64 ...
    wb       (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (109444, 6)
input shape should be (109444, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (109444, 12, 3, 3)
Test Files:
<xarray.Dataset>
Dimensions:  (z: 64, y: 32, x: 32, time: 3)
Coordinates:
  * z        (z) float64 0.5648 0.8473 1.13 1.412 ... 17.51 17.79 18.07 18.36
  * y        (y) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * x        (x) float64 0.4279 1.284 2.14 2.995 ... 24.39 25.25 26.1 26.96
  * time     (time) int64 1368000 1377000 1386000
Data variables:
    u        (z, y, x, time) float64 ...
    v        (z, y, x, time) float64 ...
    w        (z, y, x, time) float64 ...
    tau11    (z, y, x, time) float64 ...
    tau22    (z, y, x, time) float64 ...
    tau33    (z, y, x, time) float64 ...
    tau12    (z, y, x, time) float64 ...
    tau13    (z, y, x, time) float64 ...
    tau23    (z, y, x, time) float64 ...
    b        (z, y, x, time) float64 ...
    p        (z, y, x, time) float64 ...
output shape is (141312, 6)
input shape should be (141312, 4, 3, 3, 3)
input shape to do 3rd dimension as channel in R2Conv is (141312, 12, 3, 3)
Lossweights:
[  311258.519   1664510.3809  7294135.8963  1095282.368  10347816.8595  5265655.4457]
0
[0.01]
LR:  None
train loss: 0.29850397714673005
validation loss: 0.9103181845752709
test loss: 0.9043256468930793
1
[0.001]
LR:  None
train loss: 0.28491890823567145
validation loss: 0.8725465771656841
test loss: 0.8634807879506514
2
[0.0001]
LR:  None
train loss: 0.2836799168292595
validation loss: 0.869164152853513
test loss: 0.8609845497941443
3
[0.0001]
LR:  None
train loss: 0.2831574996720699
validation loss: 0.8673551690498749
test loss: 0.8600471166048934
4
[0.0001]
LR:  None
train loss: 0.28260936979996043
validation loss: 0.8664643056349678
test loss: 0.8577941123733449
5
[0.0001]
LR:  None
train loss: 0.2819646433664735
validation loss: 0.8654772497398022
test loss: 0.8571643815786861
6
[0.0001]
LR:  None
train loss: 0.28130992484100464
validation loss: 0.8637921254675686
test loss: 0.861583126795605
7
[0.0001]
LR:  None
train loss: 0.28099094187090135
validation loss: 0.8618777911258796
test loss: 0.8573042965151421
8
[0.0001]
LR:  None
train loss: 0.2800477421793281
validation loss: 0.8608355209803953
test loss: 0.8549741283532819
9
[0.0001]
LR:  None
train loss: 0.27943049215107496
validation loss: 0.8586466162536887
test loss: 0.8495020956232503
10
[0.0001]
LR:  None
train loss: 0.27875788558878345
validation loss: 0.8587114235485072
test loss: 0.8541725703544741
11
[0.0001]
LR:  None
train loss: 0.27810996996597637
validation loss: 0.8572960113653053
test loss: 0.8515692228955948
12
[0.0001]
LR:  None
train loss: 0.2772881218823255
validation loss: 0.8542235148631764
test loss: 0.8479724748831818
13
[0.0001]
LR:  None
train loss: 0.2767385710095898
validation loss: 0.852908507093397
test loss: 0.8470837411794406
14
[0.0001]
LR:  None
train loss: 0.27581584839390744
validation loss: 0.8507792407187593
test loss: 0.8443396840479148
15
[0.0001]
LR:  None
train loss: 0.2751218026907409
validation loss: 0.8501614102364077
test loss: 0.8447192555100234
16
[0.0001]
LR:  None
train loss: 0.27446910976284117
validation loss: 0.848528211005606
test loss: 0.8417236949995156
17
[0.0001]
LR:  None
train loss: 0.2737658692482049
validation loss: 0.8474571898812825
test loss: 0.8404219220603715
18
[0.0001]
LR:  None
train loss: 0.2731031397081788
validation loss: 0.8451600654720274
test loss: 0.8384301270706372
19
[0.0001]
LR:  None
train loss: 0.27246151555271286
validation loss: 0.8436504144925693
test loss: 0.8350663548255575
20
[0.0001]
LR:  None
train loss: 0.27190443302038203
validation loss: 0.842448138957019
test loss: 0.8356435963622844
21
[0.0001]
LR:  None
train loss: 0.2711842173052683
validation loss: 0.8408768753048556
test loss: 0.8370190596764737
22
[0.0001]
LR:  None
train loss: 0.27058700947439684
validation loss: 0.8390790423565577
test loss: 0.8333498308500565
23
[0.0001]
LR:  None
train loss: 0.2700924207425683
validation loss: 0.8387606237638292
test loss: 0.831167281883909
24
[0.0001]
LR:  None
train loss: 0.2694445213066236
validation loss: 0.8362311681019701
test loss: 0.8335386907918289
25
[0.0001]
LR:  None
train loss: 0.268879769498336
validation loss: 0.8352721940941531
test loss: 0.8280676974201218
26
[0.0001]
LR:  None
train loss: 0.26807121419110264
validation loss: 0.8343862816301797
test loss: 0.8248014618186676
27
[0.0001]
LR:  None
train loss: 0.2674169991483831
validation loss: 0.8325000117916643
test loss: 0.8306300234041215
28
[0.0001]
LR:  None
train loss: 0.2667889087318765
validation loss: 0.8314391359730176
test loss: 0.8256686578138992
29
[0.0001]
LR:  None
train loss: 0.26604128866732757
validation loss: 0.8282381697803176
test loss: 0.8222242965872262
30
[0.0001]
LR:  None
train loss: 0.26536721510878375
validation loss: 0.827116243050757
test loss: 0.8215161506347162
31
[0.0001]
LR:  None
train loss: 0.2647402734856259
validation loss: 0.8244046213692574
test loss: 0.8180546550341974
32
[0.0001]
LR:  None
train loss: 0.2641618916553917
validation loss: 0.8228878725705522
test loss: 0.8133041378344916
33
[0.0001]
LR:  None
train loss: 0.26349709090387363
validation loss: 0.8212364885613205
test loss: 0.81729969966504
34
[0.0001]
LR:  None
train loss: 0.262805998332701
validation loss: 0.8198153057360865
test loss: 0.81397610650367
35
[0.0001]
LR:  None
train loss: 0.26226426991857055
validation loss: 0.8176511968447154
test loss: 0.8088929715219242
36
[0.0001]
LR:  None
train loss: 0.2618580981507876
validation loss: 0.8169437236925953
test loss: 0.8105848930223563
37
[0.0001]
LR:  None
train loss: 0.26122032228221453
validation loss: 0.8148485327283622
test loss: 0.8087175031044326
38
[0.0001]
LR:  None
train loss: 0.2606589866759173
validation loss: 0.8142729268318455
test loss: 0.8078720030331134
39
[0.0001]
LR:  None
train loss: 0.2602210091787168
validation loss: 0.8126299641036838
test loss: 0.8027019744557333
40
[0.0001]
LR:  None
train loss: 0.2597516580409873
validation loss: 0.811665605020595
test loss: 0.8046972781018309
41
[0.0001]
LR:  None
train loss: 0.2591174678148889
validation loss: 0.8103296274617369
test loss: 0.8041243076803559
42
[0.0001]
LR:  None
train loss: 0.25884744156417766
validation loss: 0.8090191248326598
test loss: 0.8025313983953094
43
[0.0001]
LR:  None
train loss: 0.2583671443913821
validation loss: 0.8086498142046401
test loss: 0.801306875113413
44
[0.0001]
LR:  None
train loss: 0.2578569106900405
validation loss: 0.8067436423097478
test loss: 0.7984189566019854
45
[0.0001]
LR:  None
train loss: 0.2574551106757614
validation loss: 0.8063484876453977
test loss: 0.7981649139982384
46
[0.0001]
LR:  None
train loss: 0.2570099627933095
validation loss: 0.8052319628606097
test loss: 0.7995393724931928
47
[0.0001]
LR:  None
train loss: 0.2567239863135996
validation loss: 0.8035703449045591
test loss: 0.7958904152415903
48
[0.0001]
LR:  None
train loss: 0.2562740979110115
validation loss: 0.8042495033483121
test loss: 0.7961491649249093
49
[0.0001]
LR:  None
train loss: 0.2558364750335484
validation loss: 0.8041654891827125
test loss: 0.7980198632442077
50
[0.0001]
LR:  None
train loss: 0.25536500265378076
validation loss: 0.8016212176724841
test loss: 0.7946922530263945
51
[0.0001]
LR:  None
train loss: 0.25509484625227224
validation loss: 0.8010975670009963
test loss: 0.794209582128112
52
[0.0001]
LR:  None
train loss: 0.25482222993047393
validation loss: 0.8007432360573594
test loss: 0.7937166684864785
53
[0.0001]
LR:  None
train loss: 0.2543511389253149
validation loss: 0.8004761412102283
test loss: 0.7931024859341251
54
[0.0001]
LR:  None
train loss: 0.25397108357507586
validation loss: 0.7986377215811102
test loss: 0.7916052355926596
55
[0.0001]
LR:  None
train loss: 0.25371148657904363
validation loss: 0.7982322497043326
test loss: 0.7913952670049211
56
[0.0001]
LR:  None
train loss: 0.25328235956059014
validation loss: 0.7972525775035034
test loss: 0.7922228491956635
57
[0.0001]
LR:  None
train loss: 0.25302549359498117
validation loss: 0.7975994966856312
test loss: 0.790027039450343
58
[0.0001]
LR:  None
train loss: 0.25256024060042304
validation loss: 0.7957982123460983
test loss: 0.7932745164883086
59
[0.0001]
LR:  None
train loss: 0.25237858820744824
validation loss: 0.7965791935645531
test loss: 0.7917930664306935
60
[0.0001]
LR:  None
train loss: 0.2519206147670723
validation loss: 0.7955217145834105
test loss: 0.7922720386613705
61
[0.0001]
LR:  None
train loss: 0.2515778510923436
validation loss: 0.7943699619380244
test loss: 0.7905261188447665
62
[0.0001]
LR:  None
train loss: 0.2513948675940531
validation loss: 0.7941365362269519
test loss: 0.7882483597587008
63
[0.0001]
LR:  None
train loss: 0.2511457476913016
validation loss: 0.7938514591833413
test loss: 0.7868349383561468
64
[0.0001]
LR:  None
train loss: 0.25072345617751207
validation loss: 0.7922707579867078
test loss: 0.7860233246827433
65
[0.0001]
LR:  None
train loss: 0.25034327952736685
validation loss: 0.7922909638560828
test loss: 0.7852351862182672
66
[0.0001]
LR:  None
train loss: 0.24994528774641148
validation loss: 0.7911910996139799
test loss: 0.785792467726291
67
[0.0001]
LR:  None
train loss: 0.24957363301678082
validation loss: 0.7912490367461125
test loss: 0.783904700354937
68
[0.0001]
LR:  None
train loss: 0.24925658780038876
validation loss: 0.7904693404038305
test loss: 0.7857281367824336
69
[0.0001]
LR:  None
train loss: 0.24887544607361317
validation loss: 0.7894577798830383
test loss: 0.7850183569161618
70
[0.0001]
LR:  None
train loss: 0.24857020802373736
validation loss: 0.7889758011960185
test loss: 0.7840145902009991
71
[0.0001]
LR:  None
train loss: 0.248296195728995
validation loss: 0.78919147039582
test loss: 0.7829333735456615
72
[0.0001]
LR:  None
train loss: 0.24810128046498992
validation loss: 0.787598484366628
test loss: 0.7832141250888961
73
[0.0001]
LR:  None
train loss: 0.24767216546987586
validation loss: 0.7873830237031686
test loss: 0.7807239367192358
74
[0.0001]
LR:  None
train loss: 0.24724789600665137
validation loss: 0.786729725806457
test loss: 0.7826946507732722
75
[0.0001]
LR:  None
train loss: 0.24681765054360824
validation loss: 0.7865132823580029
test loss: 0.7780732503808903
76
[0.0001]
LR:  None
train loss: 0.24675793560012566
validation loss: 0.7855922541859108
test loss: 0.7833354330353057
77
[0.0001]
LR:  None
train loss: 0.24629102853693502
validation loss: 0.7862282881081315
test loss: 0.7830406093665175
78
[0.0001]
LR:  None
train loss: 0.24601417845423795
validation loss: 0.7826358720564077
test loss: 0.7780235204316055
79
[0.0001]
LR:  None
train loss: 0.2455987665095212
validation loss: 0.7826942718877903
test loss: 0.7772349886399587
80
[0.0001]
LR:  None
train loss: 0.24512862385909073
validation loss: 0.7835860510430428
test loss: 0.7775017955218947
81
[0.0001]
LR:  None
train loss: 0.24479228895996413
validation loss: 0.7809564281067365
test loss: 0.7760763580845026
82
[0.0001]
LR:  None
train loss: 0.24438129660514898
validation loss: 0.7807056274033228
test loss: 0.7752283885467873
83
[0.0001]
LR:  None
train loss: 0.2443857300011096
validation loss: 0.7806097900046319
test loss: 0.7728007454319286
84
[0.0001]
LR:  None
train loss: 0.2436776363269196
validation loss: 0.7785790420601185
test loss: 0.7731260035803913
85
[0.0001]
LR:  None
train loss: 0.24324714329831265
validation loss: 0.7800067393979262
test loss: 0.7761021604285532
86
[0.0001]
LR:  None
train loss: 0.24293888751749346
validation loss: 0.7779013722389673
test loss: 0.7712395844398254
87
[0.0001]
LR:  None
train loss: 0.24243997125526742
validation loss: 0.7771465275998153
test loss: 0.771833978442201
88
[0.0001]
LR:  None
train loss: 0.24230095764565807
validation loss: 0.7773433601852361
test loss: 0.7718419502317332
89
[0.0001]
LR:  None
train loss: 0.2418453246485384
validation loss: 0.7756956332847906
test loss: 0.7702526855319356
90
[0.0001]
LR:  None
train loss: 0.24121746027671825
validation loss: 0.7756992308796942
test loss: 0.7740798619992096
91
[0.0001]
LR:  None
train loss: 0.2411407595746678
validation loss: 0.7738210251671284
test loss: 0.768708876415965
92
[0.0001]
LR:  None
train loss: 0.2403877918084067
validation loss: 0.7730970797182456
test loss: 0.7702182539124931
93
[0.0001]
LR:  None
train loss: 0.2401423161120806
validation loss: 0.7723527312677388
test loss: 0.7678661554206477
94
[0.0001]
LR:  None
train loss: 0.2397308946953236
validation loss: 0.7713603237086384
test loss: 0.7642035060107684
95
[0.0001]
LR:  None
train loss: 0.23928148122573412
validation loss: 0.770690345983546
test loss: 0.7656075983107631
96
[0.0001]
LR:  None
train loss: 0.23888028814584134
validation loss: 0.7696852576921029
test loss: 0.7630821475335147
97
[0.0001]
LR:  None
train loss: 0.23848124250889183
validation loss: 0.7692975510201997
test loss: 0.7652700615081253
98
[0.0001]
LR:  None
train loss: 0.23800047781464398
validation loss: 0.7674995025357381
test loss: 0.7652274870253808
99
[0.0001]
LR:  None
train loss: 0.23768989258394893
validation loss: 0.7683814796282016
test loss: 0.7667421555402434
100
[0.0001]
LR:  None
train loss: 0.23729805093369472
validation loss: 0.765761652021996
test loss: 0.7609675938259051
101
[0.0001]
LR:  None
train loss: 0.2368545485772028
validation loss: 0.7660291590918231
test loss: 0.7619697768331862
102
[0.0001]
LR:  None
train loss: 0.23673913258781726
validation loss: 0.7658183629952564
test loss: 0.7595657788303528
103
[0.0001]
LR:  None
train loss: 0.23638998337426964
validation loss: 0.7662045727146204
test loss: 0.7596593797733581
104
[0.0001]
LR:  None
train loss: 0.2360786403571848
validation loss: 0.7649121808860733
test loss: 0.7611827268480814
105
[0.0001]
LR:  None
train loss: 0.2357559822327472
validation loss: 0.7633177131585734
test loss: 0.7563379714320251
106
[0.0001]
LR:  None
train loss: 0.23528152989946535
validation loss: 0.7640087465753581
test loss: 0.7603597239702863
107
[0.0001]
LR:  None
train loss: 0.23495692701822635
validation loss: 0.7631420150521284
test loss: 0.7592417704778589
108
[0.0001]
LR:  None
train loss: 0.23478098100565603
validation loss: 0.7630764870660977
test loss: 0.7597107163788612
109
[0.0001]
LR:  None
train loss: 0.23438567703094249
validation loss: 0.7634322889192531
test loss: 0.7588218702830583
110
[0.0001]
LR:  None
train loss: 0.23405627344294608
validation loss: 0.7612285425478053
test loss: 0.7546684046365856
111
[0.0001]
LR:  None
train loss: 0.2338218958218083
validation loss: 0.7618030961090274
test loss: 0.755121300527745
112
[0.0001]
LR:  None
train loss: 0.23371063341957482
validation loss: 0.7609068805502373
test loss: 0.7565674908467154
113
[0.0001]
LR:  None
train loss: 0.2333009890701327
validation loss: 0.761156907297822
test loss: 0.755812527144224
114
[0.0001]
LR:  None
train loss: 0.23305672855184248
validation loss: 0.7592349078870427
test loss: 0.7548188131212004
115
[0.0001]
LR:  None
train loss: 0.2330027514647512
validation loss: 0.7622869459315469
test loss: 0.758056806505334
116
[0.0001]
LR:  None
train loss: 0.23240120780046786
validation loss: 0.7593573276564975
test loss: 0.7536647011067613
117
[0.0001]
LR:  None
train loss: 0.23215027829367754
validation loss: 0.7594735006577772
test loss: 0.7569578757374521
118
[0.0001]
LR:  None
train loss: 0.23203819534869555
validation loss: 0.7595486164770162
test loss: 0.7543214573780241
119
[0.0001]
LR:  None
train loss: 0.23171561239206198
validation loss: 0.7588886327868254
test loss: 0.7520936810987512
120
[0.0001]
LR:  None
train loss: 0.2313923096990509
validation loss: 0.7592672040537966
test loss: 0.7534500804816247
121
[0.0001]
LR:  None
train loss: 0.23117265297677494
validation loss: 0.7587589740984567
test loss: 0.7536835663936412
122
[0.0001]
LR:  None
train loss: 0.23081564322199524
validation loss: 0.7574025534673359
test loss: 0.7506973814816098
123
[0.0001]
LR:  None
train loss: 0.2306834345597589
validation loss: 0.7584046243929545
test loss: 0.753038552319674
124
[0.0001]
LR:  None
train loss: 0.23029787132363128
validation loss: 0.7569544007713819
test loss: 0.7501683589649123
125
[0.0001]
LR:  None
train loss: 0.23031375142777089
validation loss: 0.7584266652230097
test loss: 0.753116927932561
126
[0.0001]
LR:  None
train loss: 0.2298671219198063
validation loss: 0.757388459646277
test loss: 0.7539267530819871
127
[0.0001]
LR:  None
train loss: 0.2297294462037175
validation loss: 0.7569330171380013
test loss: 0.7504656554408733
128
[0.0001]
LR:  None
train loss: 0.2294990566911151
validation loss: 0.7572536739624802
test loss: 0.7518384336440199
129
[0.0001]
LR:  None
train loss: 0.22916464451458893
validation loss: 0.7562866010626094
test loss: 0.7544548167129344
130
[0.0001]
LR:  None
train loss: 0.2289829266079951
validation loss: 0.7568586179625145
test loss: 0.7499418277150949
131
[0.0001]
LR:  None
train loss: 0.22891293711479357
validation loss: 0.7563080649131921
test loss: 0.7527385604260698
132
[0.0001]
LR:  None
train loss: 0.22877887823806245
validation loss: 0.7568076464816886
test loss: 0.7490969835258419
133
[0.0001]
LR:  None
train loss: 0.22840486472377786
validation loss: 0.7555932316934096
test loss: 0.7502743376284965
134
[0.0001]
LR:  None
train loss: 0.22830864681257168
validation loss: 0.7568226412903334
test loss: 0.7495986836323378
135
[0.0001]
LR:  None
train loss: 0.2279079009200349
validation loss: 0.7555628886150978
test loss: 0.7501945430132535
136
[0.0001]
LR:  None
train loss: 0.22767269042577323
validation loss: 0.7554432104107514
test loss: 0.752719434308826
137
[0.0001]
LR:  None
train loss: 0.22758913255124014
validation loss: 0.7558417865203928
test loss: 0.7506870468874001
138
[0.0001]
LR:  None
train loss: 0.22737313508480195
validation loss: 0.7554061915829465
test loss: 0.7499029142345255
139
[0.0001]
LR:  None
train loss: 0.22722706309802151
validation loss: 0.7543916668284965
test loss: 0.7495347484206151
140
[0.0001]
LR:  None
train loss: 0.22729921982046014
validation loss: 0.7551093041943985
test loss: 0.7494835889420898
141
[0.0001]
LR:  None
train loss: 0.22683849575803094
validation loss: 0.754201853058497
test loss: 0.7477699996542609
142
[0.0001]
LR:  None
train loss: 0.22651145728238836
validation loss: 0.7549727155663282
test loss: 0.7520552893118542
143
[0.0001]
LR:  None
train loss: 0.22627765904514124
validation loss: 0.7553384966396618
test loss: 0.7493305006227466
144
[0.0001]
LR:  None
train loss: 0.226230542926612
validation loss: 0.7567968669934789
test loss: 0.7515880328928399
145
[0.0001]
LR:  None
train loss: 0.22611451463101015
validation loss: 0.7552430554788206
test loss: 0.7496335621694684
146
[0.0001]
LR:  None
train loss: 0.2258816002017715
validation loss: 0.7547485283931141
test loss: 0.7489789763212601
147
[0.0001]
LR:  None
train loss: 0.22577356169104587
validation loss: 0.7546697200933444
test loss: 0.7498698861969829
148
[0.0001]
LR:  None
train loss: 0.22547796471170126
validation loss: 0.7553334360009456
test loss: 0.7499645095966624
149
[0.0001]
LR:  None
train loss: 0.2252362684641616
validation loss: 0.7548303426043687
test loss: 0.7509197455589846
150
[0.0001]
LR:  None
train loss: 0.22513914647694494
validation loss: 0.754867561211653
test loss: 0.7541654124168048
151
[0.0001]
LR:  None
train loss: 0.2249862949533532
validation loss: 0.7547284098067709
test loss: 0.7508724043166896
152
[0.0001]
LR:  None
train loss: 0.22494544459101073
validation loss: 0.756442981937994
test loss: 0.7538189304604962
153
[0.0001]
LR:  None
train loss: 0.2245983554596248
validation loss: 0.7536584530648096
test loss: 0.7490286988935709
154
[0.0001]
LR:  None
train loss: 0.22441172308623666
validation loss: 0.7549421103329761
test loss: 0.7509110556299069
155
[0.0001]
LR:  None
train loss: 0.22414949038542678
validation loss: 0.7550524601658997
test loss: 0.7484703073968418
156
[0.0001]
LR:  None
train loss: 0.22435135122353178
validation loss: 0.7561366041624568
test loss: 0.7497870606171817
157
[0.0001]
LR:  None
train loss: 0.22433938267254538
validation loss: 0.7558717461001315
test loss: 0.7495100638859108
158
[0.0001]
LR:  None
train loss: 0.22378149733511146
validation loss: 0.7532769332192392
test loss: 0.7473288603675243
159
[0.0001]
LR:  None
train loss: 0.22363500578875312
validation loss: 0.7536052282685873
test loss: 0.7501854207013816
160
[0.0001]
LR:  None
train loss: 0.22339399576226326
validation loss: 0.7541466171411967
test loss: 0.7499725161432818
161
[0.0001]
LR:  None
train loss: 0.22327364743657463
validation loss: 0.7538525179349249
test loss: 0.7472392821207929
162
[0.0001]
LR:  None
train loss: 0.2232092131716524
validation loss: 0.7545787785379456
test loss: 0.7501802513080394
163
[0.0001]
LR:  None
train loss: 0.22299469992664825
validation loss: 0.7543373521127081
test loss: 0.7491423150621072
164
[0.0001]
LR:  None
train loss: 0.22288863944913237
validation loss: 0.7544664958622342
test loss: 0.7478171079397039
165
[0.0001]
LR:  None
train loss: 0.22263155519457742
validation loss: 0.7545716802957446
test loss: 0.748188657549375
166
[0.0001]
LR:  None
train loss: 0.22259554610335952
validation loss: 0.7545882029238931
test loss: 0.7461695449674909
167
[0.0001]
LR:  None
train loss: 0.22247465736037905
validation loss: 0.7552926383694376
test loss: 0.7501959106568249
168
[0.0001]
LR:  None
train loss: 0.22213785132244557
validation loss: 0.7558272766144067
test loss: 0.7498470500027624
169
[0.0001]
LR:  None
train loss: 0.22202721811550608
validation loss: 0.7548306752647541
test loss: 0.7490480983838708
170
[0.0001]
LR:  None
train loss: 0.2219549382249709
validation loss: 0.7534959410450789
test loss: 0.747893649449296
171
[0.0001]
LR:  None
train loss: 0.2217731097614504
validation loss: 0.7539970467589903
test loss: 0.7497733720386478
172
[0.0001]
LR:  None
train loss: 0.22173871823843044
validation loss: 0.7544220409060203
test loss: 0.7482886744438878
173
[0.0001]
LR:  None
train loss: 0.22153551351278317
validation loss: 0.7545103416436264
test loss: 0.749566450426269
174
[0.0001]
LR:  None
train loss: 0.22133660846361364
validation loss: 0.7545126178435497
test loss: 0.7460456949199159
175
[0.0001]
LR:  None
train loss: 0.22127271379858607
validation loss: 0.7543862049877287
test loss: 0.7492877503195368
176
[0.0001]
LR:  None
train loss: 0.22101477599561045
validation loss: 0.7555911509652823
test loss: 0.7512722619053509
177
[0.0001]
LR:  None
train loss: 0.2209890479855683
validation loss: 0.754581010972894
test loss: 0.7482539843493958
178
[0.0001]
LR:  None
train loss: 0.22075553763218395
validation loss: 0.7543541156177365
test loss: 0.7482252969491995
ES epoch: 158
Test data
Skills for tau_11
R^2: 0.9302
Correlation: 0.9672

Skills for tau_12
R^2: 0.6943
Correlation: 0.8368

Skills for tau_13
R^2: 0.7277
Correlation: 0.8598

Skills for tau_22
R^2: 0.7745
Correlation: 0.8833

Skills for tau_23
R^2: 0.6770
Correlation: 0.8264

Skills for tau_33
R^2: 0.6618
Correlation: 0.8390

Validation data
Skills for tau_11
R^2: 0.9244
Correlation: 0.9645

Skills for tau_12
R^2: 0.6945
Correlation: 0.8368

Skills for tau_13
R^2: 0.7343
Correlation: 0.8628

Skills for tau_22
R^2: 0.7783
Correlation: 0.8854

Skills for tau_23
R^2: 0.6755
Correlation: 0.8260

Skills for tau_33
R^2: 0.6694
Correlation: 0.8427

Train data
Skills for tau_11
R^2: 0.9754
Correlation: 0.9878

Skills for tau_12
R^2: 0.8410
Correlation: 0.9182

Skills for tau_13
R^2: 0.6608
Correlation: 0.8181

Skills for tau_22
R^2: 0.8707
Correlation: 0.9348

Skills for tau_23
R^2: 0.7065
Correlation: 0.8420

Skills for tau_33
R^2: 0.2584
Correlation: 0.5306

[[0.9676 0.8365 0.8662 0.8861 0.834  0.8374]
 [0.9672 0.8381 0.8608 0.886  0.8324 0.8348]
 [0.9671 0.8409 0.8653 0.8842 0.8336 0.842 ]
 [0.9671 0.8402 0.8676 0.8873 0.83   0.8411]
 [0.9672 0.8368 0.8598 0.8833 0.8264 0.839 ]]
[[0.9334 0.6927 0.7414 0.7768 0.6876 0.6596]
 [0.9318 0.6969 0.733  0.7774 0.6849 0.6519]
 [0.9311 0.7016 0.7394 0.774  0.6856 0.6701]
 [0.9316 0.6994 0.7474 0.7809 0.6821 0.6685]
 [0.9302 0.6943 0.7277 0.7745 0.677  0.6618]]
tau_11 avg. R^2 is 0.9316297907986197 +/- 0.001058182916822907
tau_12 avg. R^2 is 0.6969852035090154 +/- 0.003237929225625285
tau_13 avg. R^2 is 0.7377818241080907 +/- 0.00681895244989096
tau_22 avg. R^2 is 0.7766975346900142 +/- 0.0024416552994761327
tau_23 avg. R^2 is 0.6834340490172222 +/- 0.003687551327221093
tau_33 avg. R^2 is 0.6623856491754363 +/- 0.006547572347174232
Overall avg. R^2 is 0.7481523418830665 +/- 0.0027208035296957178
